<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://sanjanad1024.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sanjanad1024.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-22T14:48:17+00:00</updated><id>https://sanjanad1024.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal page of Sanjana Das. </subtitle><entry><title type="html">The Lovász Local Lemma</title><link href="https://sanjanad1024.github.io/blog/2024/lovasz-local-lemma/" rel="alternate" type="text/html" title="The Lovász Local Lemma"/><published>2024-08-06T00:00:00+00:00</published><updated>2024-08-06T00:00:00+00:00</updated><id>https://sanjanad1024.github.io/blog/2024/lovasz-local-lemma</id><content type="html" xml:base="https://sanjanad1024.github.io/blog/2024/lovasz-local-lemma/"><![CDATA[<p>In this post, I’ll explain the statement and proof of the Lovász local lemma.</p> <h1 id="motivation">Motivation</h1> <p>There are many situations where we have a random process and a collection of ‘bad’ events $A_1$, $\ldots$, $A_n$, and we want to show that there’s a positive probability that none of these bad events happen — i.e., that</p> <div> \[\mathbb{P}[\overline{A_1} \wedge \cdots \wedge \overline{A_n}] &gt; 0.\] </div> <p>This especially happens when we’re using the probabilistic method — often we’re trying to prove the existence of an object with certain properties by taking a <em>random</em> construction and showing that it works with positive probability, and in many cases, the construction working corresponds to avoiding a collection of bad events.</p> <p>There’s two extreme cases where it’s easy to show that we avoid all the bad events with positive probability. One is when there’s not too many events and their probabilities are all small (relative to the number of events). In this case, we can simply use a union bound — we have</p> <div> \[\mathbb{P}[A_1 \vee \cdots \vee A_n] \leq \mathbb{P}[A_1] + \cdots + \mathbb{P}[A_n],\] </div> <p>so if $\mathbb{P}[A_1] + \cdots + \mathbb{P}[A_n] &lt; 1$, then we get that</p> <div> \[\mathbb{P}[\overline{A_1} \wedge \cdots \wedge \overline{A_n}] = 1 - \mathbb{P}[A_1 \vee \cdots \vee A_n] &gt; 0.\] </div> <p>In particular, if we know that $\mathbb{P}[A_i] \leq p$ for all $i$ and $pn &lt; 1$, then this means we avoid all the bad events with positive probability.</p> <p>The other extreme case is when the events are all <em>independent</em>. In this case, it doesn’t matter how many events there are or how large their probabilities are (as long as they aren’t $1$, of course) — we simply have</p> <div> \[\mathbb{P}[\overline{A_1} \wedge \cdots \wedge \overline{A_n}] = \prod_{i = 1}^n (1 - \mathbb{P}[A_i]) &gt; 0.\] </div> <p>The motivation for the Lovász local lemma is the question of whether we can ‘interpolate’ between these two extreme cases.</p> <div class="question"> What if we have lots of events, but each is only dependent on a few others (where the event probabilities are small relative to the number of <em>dependencies</em>, rather than <em>events</em>)? Can we still avoid all the events with positive probability? </div> <p>It turns out the answer is yes! And that’s exactly what the Lovász local lemma states.</p> <h1 id="the-lovász-local-lemma">The Lovász local lemma</h1> <p>To state the Lovász local lemma, we first need to formalize what it means for each event to only be dependent on a few others.</p> <div class="definition"> We say an event $A$ is <span class="vocab">independent</span> from a collection of events $\{A_1, \ldots, A_n\}$ if conditioning on any logical combination of the events $A_1$, $\ldots$, $A_n$ doesn't affect the probability of $A$. </div> <p>For example, this means $\mathbb{P}[A \mid A_1 \wedge \overline{A_3} \wedge A_5] = \mathbb{P}[A]$.</p> <p>We’ll first state a ‘symmetric’ form of the Lovász local lemma (which is often what’s used in applications when our bad events have comparable probabilities).</p> <div class="theorem" text="Symmetric Lov&aacute;sz local lemma"> Let $A_1$, $\ldots$, $A_n$ be events such that $\mathbb{P}[A_i] \leq p$ for all $i$, and such that each event $A_i$ is independent from a collection of all but $d$ others. If $ep(d + 1) \leq 1$, then \[\mathbb{P}[\overline{A_1} \wedge \cdots \wedge \overline{A_n}] &gt; 0.\] </div> <p>(Here $e$ denotes the constant $e \approx 2.718$.)</p> <p>The important feature of this theorem is that the condition $ep(d + 1) \leq 1$ doesn’t reference the number of <em>events</em> at all — it only considers the number of <em>dependencies</em>.</p> <p>We’ll actually deduce the symmetric form of the Lovász local lemma from a more general ‘asymmetric’ form (which is useful even when the bad events <em>don’t</em> have comparable probabilities).</p> <div class="theorem" text="Asymmetric Lov&aacute;sz local lemma"> Let $A_1$, $\ldots$, $A_n$ be a collection of events, and for each $i \in [n]$, let $\mathcal{D}_i \subseteq [n]$ be a set such that $A_i$ is independent from the collection of events $\{A_j \mid j \not\in \mathcal{D}_i\}$. <br/><br/> Suppose there exist $x_1, \ldots, x_n \in (0, 1)$ such that $\mathbb{P}[A_i] \leq x_i\prod_{j \in \mathcal{D}_i} (1 - x_j)$ for all $i \in [n]$. Then \[\mathbb{P}[\overline{A_1} \wedge \cdots \wedge \overline{A_n}] \geq \prod_{i = 1}^n (1 - x_i) &gt; 0.\] </div> <p>First, here’s why the asymmetric version of the Lovász local lemma implies the symmetric version — given $A_1$, $\ldots$, $A_n$ as in the symmetric version, we can set $x_i = 1/(d + 1)$ for each $i$. This satisfies the hypothesis of the asymmetric version — for each $i$, we have $\lvert \mathcal{D}_i \rvert \leq d$, so</p> <div> \[x_i\prod_{j \in \mathcal{D}_i}(1 - x_j) \geq \frac{1}{d + 1}\left(1 - \frac{1}{d + 1}\right)^d \geq \frac{1}{(d + 1)e},\] </div> <p>and the assumption that $ep(d + 1) \leq 1$ means that this is at least $p$ (which was an upper bound on $\mathbb{P}[A_i]$). So the asymmetric version tells us we avoid all the bad events with positive probability, as desired.</p> <p>In the rest of this post, we’ll prove this asymmetric version of the Lovász local lemma.</p> <h1 id="proof-of-the-asymmetric-version">Proof of the asymmetric version</h1> <p>We’re going to actually prove the following claim.</p> <div class="claim"> For all $\mathcal{S} \subseteq [n]$ and $i \not\in \mathcal{S}$, we have $\mathbb{P}[A_i \mid \bigwedge_{j \in \mathcal{S}} \overline{A_j}] \leq x_i$. </div> <p>Once we have this claim, we can deduce <span class="crossref">Theorem 4</span> by writing</p> <div class="eqn"> \[\mathbb{P}[\overline{A_1} \wedge \cdots \wedge \overline{A_n}] = \prod_{i = 1}^n \mathbb{P}[\overline{A_i} \mid \overline{A_1} \wedge \cdots \wedge \overline{A_{i - 1}}] \leq \prod_{i = 1}^n (1 - x_i).\] </div> <p>We’ll prove this claim by induction on $\lvert\mathcal{S}\rvert$. When $\mathcal{S}$ is empty, we have $\mathbb{P}[A_i] \leq x_i\prod_{j \in \mathcal{D}_i} (1 - x_j) \leq x_i$, so the claim is true. Now suppose that $\mathcal{S}$ is not empty, and that we’ve proven the claim for all smaller sets $\mathcal{S}’$.</p> <p>We’re trying to consider the probability of $A_i$ conditioned on a bunch of events, so we’ll start by splitting these events into two — we’ll separately consider the ones that are independent from $A_i$ and the ones that aren’t. So we split up the event that we’re trying to condition on as $B_{\text{ind}} \wedge B_{\text{dep}}$, where</p> <div> \[B_{\text{ind}} = \bigwedge_{j \in \mathcal{S} \setminus \mathcal{D}_i} \overline{A_j} \quad \text{and} \quad B_{\text{dep}} = \bigwedge_{j \in \mathcal{S} \cap \mathcal{D}_i} \overline{A_j}.\] </div> <p>First, since $A_i$ is independent of \(\{A_j \mid j \not\in \mathcal{D}_i\}\) and $B_{\text{ind}}$ is a logical combination of these events, we know that conditioning on $B_{\text{ind}}$ doesn’t affect the probability of $A_i$ — i.e., we have $\mathbb{P}[A_i \mid B_{\text{ind}}] = \mathbb{P}[A_i]$. But we <em>don’t</em> know anything about how conditioning on $B_{\text{dep}}$ affects $A_i$, so the best thing we can do is try to get rid of it — so we drop the conditioning on $B_{\text{dep}}$ using the fact that</p> <div class="eqn"> \[\mathbb{P}[A_i \mid B_{\text{ind}} \wedge B_{\text{dep}}] = \frac{\mathbb{P}[A_i \wedge B_{\text{dep}} \mid B_{\text{ind}}]}{\mathbb{P}[B_{\text{dep}}]} \leq \frac{\mathbb{P}[A_i \mid B_{\text{ind}}]}{\mathbb{P}[B_{\text{dep}}]}.\] </div> <p>(We can’t really do anything better with $B_{\text{dep}}$ because we know nothing about how the events $A_j$ that go into it interact with $A_i$.)</p> <p>Now in the numerator, we’re only conditioning on $B_{\text{ind}}$, and we know how to handle this conditioning — we simply have $\mathbb{P}[A_i \mid B_{\text{ind}}] = \mathbb{P}[A_i]$. So now it remains to get a <em>lower</em> bound on the denominator $\mathbb{P}[B_{\text{dep}}]$.</p> <p>But $B_{\text{dep}}$ is an event of the form \(\overline{A_{j_1}} \wedge \cdots \wedge \overline{A_{j_m}}\) (where $j_1$, $\ldots$, $j_m$ are the elements of $\mathcal{S} \cap \mathcal{D}_i$), so we can actually do this in the same way as in <span class="crossref">(1)</span> using the inductive hypothesis! We can write</p> <div class="eqn"> \[\mathbb{P}[B_{\text{dep}}] = \prod_{k = 1}^m \mathbb{P}[\overline{A_{j_k}} \mid \overline{A_{j_1}} \wedge \cdots \wedge \overline{A_{j_{k - 1}}}].\] </div> <p>And for each term on the right-hand side, the set of indices of the events we’re conditioning on is a proper subset of $\mathcal{S} \cap \mathcal{D}_i \subseteq \mathcal{S}$, which means in particular that we’ve already proven the claim for this set (in our induction). So for each $k$, we have</p> <div> \[\mathbb{P}[A_{j_k} \mid \overline{A_{j_1}} \wedge \cdots \wedge \overline{A_{j_{k - 1}}}] \leq x_{j_k},\] </div> <p>which means that the $k$th term on the right-hand side of <span class="crossref">(3)</span> is at least $1 - x_{j_k}$. This gives the lower bound</p> <div> \[\mathbb{P}[B_{\text{dep}}] \geq \prod_{k = 1}^m (1 - x_{j_k}) = \prod_{j \in \mathcal{S} \cap \mathcal{D}_i} (1 - x_j) \geq \prod_{j \in \mathcal{D}_i} (1 - x_j),\] </div> <p>Plugging this into <span class="crossref">(2)</span>, we get that</p> <div> \[\mathbb{P}[A_i \mid B_{\text{ind}} \wedge B_{\text{dep}}] \leq \frac{\mathbb{P}[A_i]}{\prod_{j \in \mathcal{D}_i} (1 - x_j)} \leq x_i\] </div> <p>(using the bound on $\mathbb{P}[A_i]$ from the given hypothesis), as desired.</p> <div class="remark"> I don't really know how you'd come up with this proof, but I would imagine that you start with the statement of <span class="crossref">Claim 5</span> rather than <span class="crossref">Theorem 4</span> &mdash; you want to find <em>some</em> condition on $\mathbb{P}[A_1]$, $\ldots$, $\mathbb{P}[A_n]$ that ensures $\mathbb{P}[\overline{A_1} \wedge \cdots \wedge \overline{A_n}] &gt; 0$, and one way to ensure this is to try to control the conditional probabilities $\mathbb{P}[A_i \mid \overline{A_1} \wedge \cdots \wedge \overline{A_{i - 1}}]$ (and if we're trying to control these conditional probabilities, we might as well try to control conditional probabilities with more general index sets). <br/><br/> Once we have the statement of <span class="crossref">Claim 5</span>, the rest of the proof is reasonably intuitive &mdash; we split the event we're conditioning on into a part that we know how to deal with ($B_{\text{ind}}$, which $A_i$ is independent from) and a part we don't know how to deal with ($B_{\text{dep}}$, whose interaction with $A_i$ we can't really say anything about), and get rid of the latter by dropping conditioning (which costs us a factor of $\mathbb{P}[B_{\text{dep}}]$). Then we need a lower bound on $\mathbb{P}[B_{\text{dep}}]$, and if we expand out this probability as a product of conditional probabilities, we realize that we can get a lower bound on each by using induction (each of the terms in the product is of the same form as the one we're trying to bound, but with fewer events being conditioned on). <br/><br/> And when we plug in that bound, we find that the condition on $\mathbb{P}[A_i]$ that we need to make the proof of the claim go through is precisely the one in <span class="crossref">Theorem 4</span>. </div>]]></content><author><name></name></author><category term="combinatorics"/><category term="probabilistic-method"/><summary type="html"><![CDATA[A proof of the Lov&aacute;sz local lemma, which states that if we have a collection of bad events with only a few 'local' dependencies, then we avoid all of them with positive probability.]]></summary></entry><entry><title type="html">Anticoncentration of Random Subset Sums</title><link href="https://sanjanad1024.github.io/blog/2024/anticoncentration/" rel="alternate" type="text/html" title="Anticoncentration of Random Subset Sums"/><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://sanjanad1024.github.io/blog/2024/anticoncentration</id><content type="html" xml:base="https://sanjanad1024.github.io/blog/2024/anticoncentration/"><![CDATA[<p>I took 18.204 (Seminar in Discrete Math) this semester; this is a class where we each choose a topic and give a few presentations on it. My topic was anticoncentration. In this post, I’ll talk about a theorem that’s in some sense the starting point for all the things I presented on. (I may post about those things later.)</p> <h1 id="introduction">Introduction</h1> <p>In this post, we’ll consider the following question.</p> <div class="question"> Suppose we have $n$ nonzero real numbers $a_1$, $\ldots$, $a_n$, and we choose a random subset sum of these numbers &mdash; i.e., we consider the random variable $\varepsilon_1a_1 + \cdots + \varepsilon_na_n$ for $\varepsilon_i \in \{0, 1\}$ chosen uniformly and independently at random. How concentrated can this random variable be at a single point? </div> <p>In particular, we’re interested in proving <em>upper</em> bounds on how concentrated $\varepsilon_1a_1 + \cdots + \varepsilon_na_n$ can be at a single point $a$ — this means we want to upper-bound $\max_a \mathbb{P}[\varepsilon_1a_1 + \cdots + \varepsilon_na_n = a]$. This is why the topic is referred to as <span class="vocab">anticoncentration</span> — we’re trying to show that a random variable is <em>not</em> super concentrated at any point.</p> <p>This question was first considered by Littlewood and Offord (who were studying real roots of random polynomials) in a <a href="https://www.mathnet.ru/links/31fb3a11135873de2d81e19d30ad43aa/sm6161.pdf" target="_blank">paper</a> from 1943; they proved the following upper bound.</p> <div class="theorem" text="Littlewood&ndash;Offord 1943"> For all nonzero $a_1, \ldots, a_n \in \mathbb{R}$ and all $a \in \mathbb{R}$, we have \[\mathbb{P}[\varepsilon_1a_1 + \cdots + \varepsilon_na_n = a] \leq \frac{c\log n}{\sqrt{n}}\] (for some absolute constant $c &gt; 0$). </div> <p>This was later improved by Erdős to the following bound.</p> <div class="theorem" text="Erd&#337;s 1945"> For all nonzero $a_1, \ldots, a_n \in \mathbb{R}$ and all $a \in \mathbb{R}$, we have \[\mathbb{P}[\varepsilon_1a_1 + \cdots + \varepsilon_na_n = a] \leq \frac{\binom{n}{\lfloor n/2\rfloor}}{2^n}.\] </div> <p>As some intuition regarding how big the right-hand side is, by Stirling’s approximation $\binom{n}{\lfloor n/2\rfloor}$ is roughly $\frac{2^n}{\sqrt{n}}$ (ignoring constant factors), so this essentially removes the $\log n$ from the bound of Littlewood–Offord.</p> <p>Also, a nice thing is that this theorem is tight – the following construction achieves equality.</p> <div class="example"> Suppose that $a_1 = \cdots = a_n = 1$. Then for every $k$, we have \[\mathbb{P}[\varepsilon_1a_1 + \cdots + \varepsilon_n = k] = \frac{\binom{n}{k}}{2^n}\] (since getting a subset sum of $k$ corresponds to choosing exactly $k$ of the $\varepsilon_i$'s to be $1$). </div> <p>So the bound in <span class="crossref">Theorem 2</span> is the best bound we could possibly hope to get, and it turns out to be true — this essentially means that $\varepsilon_1a_1 + \cdots + \varepsilon_na_n$ is the most concentrated in the case where all the $a_i$’s are equal.</p> <p>Erdős proved this theorem using a very neat combinatorial argument, which I’ll explain in the rest of this post.</p> <h1 id="sperners-theorem">Sperner’s theorem</h1> <p>The proof is going to involve Sperner’s theorem regarding antichains of subsets of $[n]$, so we’ll first state and prove this theorem.</p> <div class="definition"> We say a collection $\mathcal{S}$ of subsets of $[n]$ is an <span class="vocab">antichain</span> if there do not exist any two distinct sets $A, B \in \mathcal{S}$ such that $A \subseteq B$. </div> <div class="example"> Letting $n = 4$, the collection of subsets $\{1, 2\}$, $\{1, 3, 4\}$, $\{2, 3, 4\}$ form an antichain (as no one of these subsets contains another). <br/><br/> On the other hand, $\{2, 4\}$, $\{1, 3, 4\}$, $\{2, 3, 4\}$ doesn't form an antichain, as the third contains the first. </div> <p>Sperner’s theorem considers the following question.</p> <div class="question"> Given $n$, what's the largest possible antichain of subsets of $[n]$? </div> <p>(By the size of an antichain $\mathcal{S}$, we mean the number of subsets in $\mathcal{S}$.)</p> <p>One potential construction of an antichain is to take all subsets of a fixed size $k$.</p> <div class="example"> For any $0 \leq k \leq n$, the collection $\mathcal{S} = \{A \subseteq [n] \mid \lvert A \rvert = k\}$ is an antichain of size $\binom{n}{k}$. </div> <p>To maximize the size of this antichain, we should take $k = \lfloor n/2\rfloor$. And Sperner’s theorem says essentially that this is the best we can do.</p> <div class="theorem" text="Sperner"> If $\mathcal{S}$ is an antichain of subsets of $[n]$, then $\lvert \mathcal{S} \rvert \leq \binom{n}{\lfloor n/2\rfloor}$. </div> <p>There are several proofs of this theorem; here’s a very cute one that I saw in 18.226 (involving a clever application of probabilistic methods).</p> <div class="proof"> Imagine we start with the set $\emptyset$ and add elements one at a time until we reach $[n]$ &mdash; so we start with the set $I_0 = \emptyset$, then add a random element $i_1 \in [n]$ to it to get the set $I_1 = \{i_1\}$, then add another random element $i_2 \in [n] \setminus \{i_1\}$ to get the set $I_2 = \{i_1, i_2\}$, and so on, until we've added all elements to finally end up with $I_n = \{i_1, \ldots, i_n\} = [n]$. This gives us a random sequence of sets $I_0 \subseteq I_1 \subseteq \cdots \subseteq I_n$. <br/><br/> On one hand, no matter what order we choose elements in, at most one set in $\mathcal{S}$ can appear in this sequence &mdash; if two sets $A$ and $B$ appeared in the sequence, then we'd have $A \subseteq B$ or $B \subseteq A$, contradicting the fact that $\mathcal{S}$ is supposed to be an antichain. <br/><br/> On the other hand, what's the <em>expected</em> number of sets $A \in \mathcal{S}$ to appear in this sequence? Each $I_k$ is a uniform random subset of $[n]$ with size $k$ (by symmetry), so if $\lvert A\rvert = k$, then the probability that $A$ appears in our sequence is \[\mathbb{P}[\text{$A$ appears in the sequence}] = \mathbb{P}[I_k = A] = \frac{1}{\binom{n}{k}}\] (since there's $\binom{n}{k}$ equally likely possibilities for $I_k$, one of which is $A$). So by linearity of expectation, the expected number of sets $A \in \mathcal{S}$ to appear in the sequence is \[\mathbb{E} \#\{A \in \mathcal{S} \mid \text{$A$ appears in the sequence}\} = \sum_{A \in \mathcal{S}} \frac{1}{\binom{n}{\lvert A\rvert}} \leq \frac{\lvert \mathcal{S}\rvert}{\binom{n}{\lfloor n/2\rfloor}}.\] And since the left-hand side is at most $1$ (the quantity we're taking the expectation of is always at most $1$), this means $\lvert \mathcal{S}\rvert \leq \binom{n}{\lfloor n/2\rfloor}$, as desired. </div> <h1 id="proof-of-the-main-theorem">Proof of the main theorem</h1> <p>Now we’re ready to prove <span class="crossref">Theorem 2</span>.</p> <p>First, we can assume that the $a_i$’s are all <em>positive</em>, as flipping the sign of some $a_i$ doesn’t affect the value of $\max_a \mathbb{P}[\varepsilon_1a_1 + \cdots + \varepsilon_na_n = a]$. One way to see this is that we could imagine choosing all the $\varepsilon_i$’s from \(\{-1, 1\}\) instead of \(\{0, 1\}\) — this wouldn’t affect $\max_a \mathbb{P}[\varepsilon_1a_1 + \cdots + \varepsilon_na_n = a]$, since it’d correspond to performing the transformation $a \mapsto 2a - \frac{1}{2}(a_1 + \cdots + a_n)$ to our random variable $\varepsilon_1a_1 + \cdots + \varepsilon_na_n$. And if we’re choosing \(\varepsilon_i \in \{-1, 1\}\), then clearly flipping the sign of $a_i$ has no effect on the distribution of this random variable.</p> <p>Now fix $a$, and define a collection $\mathcal{S}$ consisting of all the subsets of $[n]$ that correspond to subset sums of exactly $a$ — i.e., for each $(\varepsilon_1, \ldots, \varepsilon_n)$ satisfying $\varepsilon_1a_1 + \cdots + \varepsilon_na_n = a$, we place the corresponding set \(A = \{i \in [n] \mid \varepsilon_i = 1\}\) into $\mathcal{S}$.</p> <p>The key observation (and where Sperner’s theorem comes in) is the following.</p> <div class="claim"> The collection $\mathcal{S}$ is an antichain. </div> <div class="proof"> Assume for contradiction that there's two sets $A$ and $B$ in $\mathcal{S}$ with $A \subsetneq B$. This means they correspond to $(\varepsilon_1, \ldots, \varepsilon_n)$ and $(\varepsilon_1', \ldots, \varepsilon_n')$ such that $\varepsilon_i \leq \varepsilon_i'$ for all $i$, which means \[\varepsilon_1a_1 + \cdots + \varepsilon_na_n &lt; \varepsilon_1'a_1 + \cdots + \varepsilon_n'a_n.\] (If we're thinking about these quantities as subset sums of $\{a_1, \ldots, a_n\}$, then the point is that the subset sum corresponding to $B$ includes all the terms in the one corresponding to $A$, and at least one extra term; and since all the $a_i$'s are positive, this means its value must be strictly greater.) <br/><br/> But these sums are both supposed to be $a$ (by the definition of $\mathcal{S}$), so this is a contradiction. </div> <p>So then Sperner’s theorem implies that $\lvert \mathcal{S}\rvert \leq \binom{n}{\lfloor n/2\rfloor}$, which proves <span class="crossref">Theorem 2</span> (as there’s $2^n$ possible outcomes for $(\varepsilon_1, \ldots, \varepsilon_n)$, and the ‘good’ ones precisely correspond to elements of $\mathcal{S}$).</p>]]></content><author><name></name></author><category term="combinatorics"/><summary type="html"><![CDATA[A proof of Erd&#337;s's theorem that random sums of the form $\varepsilon_1a_1 + \cdots + \varepsilon_na_n$ (for random $\varepsilon_i \in \{0, 1\}$ and fixed $a_i$) cannot be too concentrated.]]></summary></entry><entry><title type="html">Adjoint of a Compact Operator</title><link href="https://sanjanad1024.github.io/blog/2024/adjoint-compact/" rel="alternate" type="text/html" title="Adjoint of a Compact Operator"/><published>2024-06-02T00:00:00+00:00</published><updated>2024-06-02T00:00:00+00:00</updated><id>https://sanjanad1024.github.io/blog/2024/adjoint-compact</id><content type="html" xml:base="https://sanjanad1024.github.io/blog/2024/adjoint-compact/"><![CDATA[<p>While studying for the 18.102 (Introduction to Functional Analysis) final this semester, I came across the following theorem, which I think is quite cool (see below for the relevant definitions).</p> <div class="theorem"> Let $X$ and $Y$ be Banach spaces, and suppose that $T \colon X \to Y$ is a compact bounded linear operator. Then its adjoint $T^* \colon Y^* \to X^*$ is also compact. </div> <p>In this post, I’ll explain a proof, based on the first answer to this <a href="https://math.stackexchange.com/questions/41432/easy-proof-adjointcompact-compact" target="_blank">Math StackExchange post</a>.</p> <h1 id="definitions-and-setup">Definitions and setup</h1> <p>First, here are some preliminary definitions (stated here for reference). Throughout this post, we work over $\mathbb{C}$.</p> <ul> <li> <div class="sd-hidden" desc="Normed linear spaces and Banach spaces"> <div class="definition"> We say $X$ is a <span class="vocab">normed linear space</span> if $X$ is a vector space together with a norm $\lVert \bullet \rVert$ satisfying the following three properties: <ul> <li> $\lVert x \rVert \geq 0$ for all $x \in X$, with equality if and only if $x = 0$.</li> <li> Homogeneity &mdash; $\lVert \alpha x\rVert = \lvert \alpha \rvert \lVert x\rVert$ for all $x \in X$ and $\alpha \in \mathbb{C}$. </li> <li> The triangle inequality &mdash; $\lVert x + y \rVert \leq \lVert x \rVert + \lVert y \rVert$ for all $x, y \in X$. </li> </ul> </div> <div class="definition"> A <span class="vocab">Banach space</span> is a complete normed linear space. </div> If $X$ is a normed linear space, then the norm induces a metric $d(x, y) = \lVert x - y\rVert$. When we say that $X$ is complete, we're referring to this norm. </div> </li> <li> <div class="sd-hidden" desc="Bounded linear operators"> Next, we'll state a few definitions regarding linear operators between normed linear spaces. <div class="definition"> If $X$ and $Y$ are normed linear spaces and $T \colon X \to Y$ is a linear operator, we say $T$ is <span class="vocab">bounded</span> if there exists a constant $c$ such that $\lVert Tx\rVert \leq c\lVert x\rVert$ for all $x \in X$. </div> <div class="definition"> If $T$ is a bounded linear operator, we define its <span class="vocab">operator norm</span> as \[\lVert T\rVert = \sup_{x \neq 0} \frac{\lVert Tx\rVert}{\lVert x\rVert}.\] </div> By homogeneity, we could equivalently define $\lVert T\rVert$ as $\sup_{\lVert x\rVert = 1} \lVert Tx\rVert$. <div class="fact"> If $X$ and $Y$ are normed linear spaces, then the set of bounded linear operators $T \colon X \to Y$ form a normed linear space as well (under the operator norm), which we denote by $\mathcal{B}(X, Y)$. Furthermore, if $Y$ is Banach, then so is $\mathcal{B}(X, Y)$. </div> </div> </li> <li> <div class="sd-hidden" desc="Dual spaces and adjoints"> <div class="definition"> For a normed linear space $X$, its <span class="vocab">dual space</span> $X^\ast$ is defined as $\mathcal{B}(X, \mathbb{C})$. We refer to elements of $X^\ast$ (which are bounded linear functions $f \colon X \to \mathbb{C}$) as <span class="vocab">functionals</span>. </div> Note that since $\mathbb{C}$ is Banach (i.e., complete), so is $X^*$ (for any normed linear space $X$). <div class="definition"> For any bounded linear operator $T \colon X \to Y$, we define its <span class="vocab">adjoint operator</span> $T^* \colon Y^* \to X^*$ as the map such that $(T^\ast g)(x) = g(Tx)$ for all $g \in Y^\ast$ and $x \in X$. </div> In other words, we're defining $T^\ast$ as the map sending each bounded linear operator $g \colon Y \to \mathbb{C}$ (which is an element of $Y^\ast$) to the bounded linear operator $g \circ T \colon X \to \mathbb{C}$ (an element of $X^\ast$). <div class="fact"> If $T$ is a bounded linear operator, then so is $T^\ast$. </div> </div> </li> <li> <div class="sd-hidden" desc="Compactness in metric spaces"> <div class="definition"> For a metric space $X$, we say a subset $M \subseteq X$ is <span class="vocab">compact</span> if every open cover of $M$ has a finite subcover &mdash; in other words, for any collection $\mathcal{U}$ of open sets whose union contains $M$, we can find a <em>finite</em> subcollection of $\mathcal{U}$ whose union still contains $M$. </div> <div class="definition"> For a metric space $X$, we say a subset $M \subseteq X$ is <span class="vocab">sequentially compact</span> if every sequence $(x_n) \subseteq M$ has a subsequence converging to some point in $M$. </div> <div class="fact"> Compactness and sequential compactness are equivalent (for metric spaces). </div> The reason there's two separate terms is because both can be defined in greater generality for general topological spaces, and there they're not necessarily equivalent. In this post, we'll make use of both notions. </div> </li> </ul> <p>The main focus of this post is a result about compact operators, so now we’ll discuss what it means for an operator to be compact. (We’ll only consider the case where we’re working with bounded linear operators between two Banach spaces.)</p> <div class="definition"> For Banach spaces $X$ and $Y$, we say a bounded linear operator $T \colon X \to Y$ is <span class="vocab">compact</span> if for every bounded sequence $(x_n) \subseteq X$, its image $(Tx_n) \subseteq Y$ has a convergent subsequence. </div> <p>There’s an equivalent characterization of when an operator is compact, which may make it more clear where the name comes from. (In the post, we’re going to make use of both characterizations.)</p> <div class="lemma"> For Banach spaces $X$ and $Y$, an operator $T \colon X \to Y$ is compact if and only if for every bounded subset $M \subseteq X$, its image $TM \subseteq Y$ has compact closure. </div> <p>We’ll use $\operatorname{Cl}(M)$ to refer to the closure of a subset $M$, so the condition here states that $\operatorname{Cl}(TM) \subseteq Y$ is compact (as a metric space) whenever $M \subseteq X$ is bounded. We’ll refer to this condition as $(\star)$.</p> <div class="proof"> First, the backwards direction is pretty direct &mdash; suppose that $T$ has the property $(\star)$. Consider some bounded sequence $(x_n) \subseteq X$, and let $M = \{x_n \mid n \in \mathbb{N}\}$ be the set it forms (so that $M$ is bounded). Then $(Tx_n)$ is contained in $\operatorname{Cl}(TM)$, which is compact by $(\star)$; so it must have a convergent subsequence. <br/><br/> Now we'll prove the forwards direction &mdash; we'll suppose that $T$ is compact (as in the original definition) and show that it satisfies $(\star)$. Fix some bounded set $M \subseteq X$. We want to show $\operatorname{Cl}(M)$ is compact, and we'll do so by showing that it's <em>sequentially</em> compact &mdash; i.e., that any $(y_n) \subseteq \operatorname{Cl}(TM)$ has a convergent subsequence whose limit is also in $\operatorname{Cl}(TM)$. <br/><br/> First, to apply the compactness of $T$, we really want to be working with a sequence in $TM$ rather than its closure, so the first step is to replace $(y_n) \subseteq \operatorname{Cl}(TM)$ with a sequence $(y_n') \subseteq TM$ that's 'close' to it &mdash; since $TM$ is dense in its closure, for each $n \in \mathbb{N}$ we can find some $y_n' \in TM$ with $\lVert y_n - y_n'\rVert &lt; \frac{1}{n}$, and since $y_n' \in TM$, we can find $x_n \in M$ with $y_n' = Tx_n$. <br/><br/> And now the sequence $(x_n)$ is bounded (as it's contained in $M$, which is bounded), so by the compactness of $T$, its image $(y_n') = (Tx_n) \subseteq Y$ must have a convergent subsequence. And since $\lVert y_n - y_n'\rVert \to 0$ (as $n \to \infty$), this means the corresponding subsequence of $(y_n)$ is convergent as well. <br/><br/> Finally, we've found a subsequence of $(y_n)$ that converges to <em>some</em> point $y \in Y$. But since $\operatorname{Cl}(TM)$ is closed and each $y_n$ is in $\operatorname{Cl}(TM)$, this means $y \in \operatorname{Cl}(TM)$ as well. This proves that $\operatorname{Cl}(TM)$ is sequentially compact (and therefore compact), so we're done. </div> <h1 id="the-proof">The proof</h1> <p>We’ll now prove the main theorem. Suppose that $T \colon X \to Y$ is compact. Our goal is to show that $T^\ast \colon Y^\ast \to X^\ast$ is compact as well; using the definition of a compact operator, this means we’ve got some bounded sequence $(g_n) \subseteq Y^\ast$, and we want to show that $(T^\ast g_n) \subseteq X^\ast$ has a convergent subsequence.</p> <h2 id="step-1--reformulating-the-conclusion">Step 1 — reformulating the conclusion</h2> <p>First, we’re going to find a sufficient condition for a sequence $(T^\ast g_n) \subseteq X^\ast$ to be convergent that’s more convenient to work with (and we’re going to find a subsequence satisfying this property instead).</p> <div class="claim"> Let $B = \{x \in X \mid \lVert x\rVert \leq 1\}$ be the closed unit ball in $X$, and let $K = \operatorname{Cl}(TB)$ be the closure of its image in $Y$. Suppose that $(g_n) \subseteq Y^\ast$ is a sequence of functionals such that for every $\varepsilon &gt; 0$, there exists $N$ such that for all $m, n \geq N$ we have \[\sup_{y \in K} \lvert g_m(y) - g_n(y)\rvert \leq \varepsilon.\] Then the sequence $(T^\ast g_n) \subseteq X^\ast$ is convergent. </div> <div class="proof"> First, it's enough to show that $(T^\ast g_n)$ is <em>Cauchy</em> &mdash; this automatically implies it's convergent, as $X^\ast$ is complete. So we want to show that for every $\varepsilon &gt; 0$, there exists $N$ such that for all $m, n \geq N$ we have $\lVert T^\ast g_m - T^\ast g_n\rVert \leq \varepsilon$. But for any functional $g$, we have \[\lVert T^\ast g\rVert = \sup_{\lVert x\rVert = 1} \lvert (T^\ast g)(x)\rvert = \sup_{\lVert x\rVert = 1} \lvert g(Tx)\rvert \leq \sup_{y \in K} \lvert g(y)\rvert\] (since if $\lVert x\rVert = 1$, then $Tx \in TB \subseteq K$). Taking $g = g_m - g_n$, we get \[\lVert T^\ast g_m - T^\ast g_n\rVert \leq \sup_{y \in K} \lvert g_m(y) - g_n(y)\rvert\] for all $m$ and $n$; so if we know the right-hand side is at most $\varepsilon$ for all $m, n \geq N$, then so is the left-hand side (with the same value of $N$). </div> <p>Note that $K$ is compact (because $T$ is compact, and $B \subseteq X$ is bounded) — this is the one place where we’re going to use the compactness of $T$. (After this, $T$ will basically disappear from the picture.)</p> <h2 id="step-2--finding-a-sequence-of-representatives">Step 2 — finding a sequence of representatives</h2> <p>Now our goal is to find a subsequence $(g_n’) \subseteq (g_n)$ satisfying the condition of <span class="crossref">Claim 4</span>. The first step towards doing so is finding a countable sequence of ‘representatives’ $y_1$, $y_2$, $\ldots$ for $K$ with certain nice properties. The reason for this is that we’re then going to construct our subsequence $(g_n’) \subseteq (g_n)$ such that it converges pointwise at each of these representatives $y_i$, and argue that this implies the desired conclusion.</p> <div class="claim"> There exists a sequence $(y_i) \subseteq K$ with the property that for every $\varepsilon &gt; 0$, there exists $N$ such that every $y \in K$ is within $\varepsilon$ of one of $y_1$, $\ldots$, $y_N$. </div> <div class="proof"> First, the compactness of $K$ implies that for every $n \in \mathbb{N}$, we can cover $K$ with a finite number of balls of radius $\frac{1}{n}$ &mdash; i.e., there exists a finite set of points $S_n \subseteq K$ such that $\bigcup_{y \in S_n} \mathbb{B}(z, \frac{1}{n}) \supseteq K$. (This is because the balls $\mathbb{B}(y, \frac{1}{n})$ over <em>all</em> points $y \in K$ form an open cover of $K$, and by the compactness of $K$, this open cover must have a finite subcover.) <br/><br/> Then we can take $(y_i)$ to consist of the points in $S_1$, then $S_2$, then $S_3$, and so on (in that order). <br/><br/> <center><img src="/assets/img/compact3.png" width="600" height="auto"/></center> <br/><br/> This will have the desired property &mdash; given any $\varepsilon &gt; 0$, we can fix $n \in \mathbb{N}$ such that $\frac{1}{n} \leq \varepsilon$. Then every $y \in K$ is within $\varepsilon$ of some point in $S_n$, so we can take $N = \lvert S_1\rvert + \cdots + \lvert S_n\rvert$. </div> <h2 id="step-3--defining-the-subsequence">Step 3 — defining the subsequence</h2> <p>Now we’re going to define our subsequence $(g_n’) \subseteq (g_n)$, so that it has the following property.</p> <div class="lemma"> There is a subsequence $(g_n') \subseteq (g_n)$ such that for each $i \in \mathbb{N}$, the sequence $(g_n'(y_i)) \subseteq \mathbb{C}$ is convergent (as $n \to \infty$). </div> <div class="proof"> We're going to use a diagonalization argument &mdash; the idea is that we'll first restrict to a subsequence along which $(g_n(y_1))$ converges, then further restrict to a subsequence along which $(g_n(y_2))$ converges, and so on; and in the end, we'll take the 'diagonal' of all these subsequences. <br/><br/> First, we know the sequence $(g_n)$ is bounded (by assumption), so there's some $c$ such that $\lVert g_n\rVert \leq c$ for all $n \in \mathbb{N}$. This means $\lvert g_n(y_1)\rvert \leq c\lVert y_1\rVert$ for all $n$, so the sequence $(g_n(y_1))$ is a bounded sequence in $\mathbb{C}$, which means it has a convergent subsequence (as it's a sequence in a compact subset of $\mathbb{C}$). So we can define a subsequence $(g_{n1}) \subseteq (g_n)$ such that $(g_{n1}(y_1))$ is convergent. <br/><br/> Next, we're going to further restrict this subsequence to deal with $y_2$ &mdash; we have $\lvert g_{n1}(y_2)\rvert \leq c\lVert y_2\rVert$ for all $n$, so the sequence $(g_{n1}(y_2))$ is also a bounded sequence in $\mathbb{C}$, which means it also has a convergent subsequence. So we can define a subsequence $(g_{n2}) \subseteq (g_{n1})$ such that $(g_{n2}(y_2))$ is convergent. <br/><br/> And we can continue doing this to get a nested list of subsequences $(g_n) \supseteq (g_{n1}) \supseteq (g_{n2}) \supseteq \cdots$ such that for each fixed $i \in \mathbb{N}$, the sequence $(g_{ni}(y_i)) \subseteq \mathbb{C}$ is convergent. <br/><br/> <center><img src="/assets/img/compact1.png" width="600" height="auto"/></center> <br/><br/> Finally, we define the subsequence $(g_n')$ by $g_n' = g_{nn}$ for each $n \in \mathbb{N}$ &mdash; so we're essentially taking the 'diagonal' of our list of nested subsequences. <br/><br/> <center><img src="/assets/img/compact2.png" width="600" height="auto"/></center> <br/><br/> (For example, in the above picture our sequence $(g_n')$ begins with $g_1$, $g_3$, $g_9$, $\ldots$.) <br/><br/> To see that this works, fix some $i \in \mathbb{N}$. Then if we take the sequence $(g_n'(y_i))$ and remove the first $i - 1$ terms, the resulting sequence is a subsequence of $(g_{ni}(y_i))$, which is convergent by construction; so this means $(g_n'(y_i))$ is convergent as well. </div> <h2 id="step-4--concluding">Step 4 — concluding</h2> <p>Finally, we’re ready to conclude — we’ll use the fact that $(y_i)$ forms a nice sequence of representatives for $K$ (from <span class="crossref">Claim 5</span>) together with the fact that $(g_n’)$ converges pointwise at each $y_i$ (from <span class="crossref">Lemma 6</span>) to conclude that $(g_n’)$ satisfies the condition described in <span class="crossref">Claim 4</span>.</p> <div class="claim"> For every $\varepsilon &gt; 0$, there exists $N$ such that for all $m, n \geq N$ and all $y \in K$, we have \[\lvert g_m'(y) - g_n'(y)\rvert \leq \varepsilon.\] </div> <div class="proof"> First, let $c$ be a constant such that $\lVert g_n\rVert \leq c$ for all $n \in \mathbb{N}$ (such $c$ exists because $(g_n)$ is bounded). <br/><br/> Now fix $\varepsilon &gt; 0$. Then using <span class="crossref">Claim 5</span>, we can first find a constant $M$ such that for every $y \in K$, there is some $i \in \{1, \ldots, M\}$ with $\lVert y - y_i\rVert \leq \frac{\varepsilon}{4c}$; fix this value of $M$. <br/><br/> Then for each $i \in \{1, \ldots, M\}$, we know the sequence $(g_n'(y_i))$ is convergent and therefore Cauchy (by <span class="crossref">Lemma 6</span>), so there is some $N_i$ such that for all $m, n \geq N_i$ we have $\lVert g_m'(y_i) - g_n'(y_i)\rVert \leq \frac{\varepsilon}{2}$. <br/><br/> Finally, let $N = \max\{N_1, \ldots, N_M\}$. To see that this has the desired property, consider any $y \in K$, and fix $y_i$ with $i \in \{1, \ldots, M\}$ such that $\lVert y - y_i\rVert \leq \frac{\varepsilon}{4c}$. Then we can write \[\lVert g_m'(y) - g_n'(y)\rVert \leq \lVert g_m'(y) - g_m'(y_i)\rVert + \lVert g_m'(y_i) - g_n'(y_i)\rVert + \lVert g_n'(y_i) - g_n'(y)\rVert\] by the triangle inequality. (The intuition here is that we know $g_m'$ and $g_n'$ should be 'close' at $y_i$, and we know $y$ should be close to $y_i$, so we should be able to bound each of these terms.) <br/><br/> For the first term, we have \[\lVert g_m'(y) - g_m'(y_i)\rVert \leq \lVert g_m'\rVert \lVert y - y_i\rVert \leq c\cdot \frac{\varepsilon}{4c} = \frac{\varepsilon}{4}\] (the first inequality is by the definition of the operator norm of $g_m'$). We can do the same for the last term to get that it's at most $\frac{\varepsilon}{4}$ as well. Finally, for the middle term, we have \[\lVert g_m'(y_i) - g_n'(y_i)\rVert \leq \frac{\varepsilon}{2},\] since we know $m, n \geq N \geq N_i$ and we defined $N_i$ to guarantee this inequality holds. <br/><br/> Putting these together gives that \[\lVert g_m'(y) - g_n'(y)\rVert \leq \frac{\varepsilon}{4} + \frac{\varepsilon}{2} + \frac{\varepsilon}{4} = \varepsilon\] for all $m, n \geq N$ and $y \in K$, as desired. </div> <p>This concludes the proof of <span class="crossref">Theorem 1</span> — we’ve started with an arbitrary bounded sequence $(g_n) \subseteq Y^*$ and obtained a subsequence $(g_n’) \subseteq (g_n)$ satisfying the condition of <span class="crossref">Claim 4</span> (using <span class="crossref">Claim 5</span> to choose a nice set of representatives, <span class="crossref">Lemma 6</span> to find a subsequence converging pointwise at each representative, and finally using the triangle inequality to get the conclusion, as stated in <span class="crossref">Claim 7</span>), which by <span class="crossref">Claim 4</span> means that the sequence $(T^\ast g_n’)$ is convergent.</p>]]></content><author><name></name></author><category term="analysis"/><category term="functional-analysis"/><summary type="html"><![CDATA[A proof of the theorem from functional analysis that the adjoint of a compact linear operator between Banach spaces is also compact.]]></summary></entry><entry><title type="html">Perfect Power Polynomials</title><link href="https://sanjanad1024.github.io/blog/2022/powerpoly/" rel="alternate" type="text/html" title="Perfect Power Polynomials"/><published>2022-06-04T00:00:00+00:00</published><updated>2022-06-04T00:00:00+00:00</updated><id>https://sanjanad1024.github.io/blog/2022/powerpoly</id><content type="html" xml:base="https://sanjanad1024.github.io/blog/2022/powerpoly/"><![CDATA[<p>(This post is based on a talk I gave at my high school’s math club.)</p> <h1 id="introduction">Introduction</h1> <p>In this post, we’ll answer the following question.</p> <div class="question"> Which integer-coefficient polynomials $P$ have the property that $P(n)$ is a perfect power for every integer $n$? </div> <p>To be more precise, we’ll use the following definitions. (Some notational conventions: we use $\mathbb{Z}[x]$ and $\mathbb{Q}[x]$ to denote the sets of polynomials with integer coefficients and rational coefficients, respectively, and we use $\nu_p(n)$ to denote the greatest power of $p$ dividing $n$.)</p> <div class="definition"> We say an integer $n$ is a <em>perfect power</em> if $n = m^k$ for integers $m$ and $k$ with $k &gt; 1$. </div> <div class="definition"> We say a polynomial $P \in \mathbb{Z}[x]$ is a <em>perfect power</em> if $P(x) = Q(x)^k$ for some polynomial $Q(x) \in \mathbb{Z}[x]$ and some integer $k &gt; 1$. </div> <p>If a polynomial $P$ is a perfect power, then $P(n)$ is a perfect power for every integer $n$ — we can write $P(n)$ as $Q(n)^k$. So it’s natural to ask whether the converse is true — if we know that $P(n)$ is a perfect power for every integer $n$, then must $P$ <em>itself</em> be a perfect power? It turns out the answer is yes.</p> <div class="theorem"> If a polynomial $P \in \mathbb{Z}[x]$ has the property that $P(n)$ is a perfect power for all integers $n$, then $P$ itself must be a perfect power. </div> <p>In this post, we’ll explain some theory regarding the behavior of integer-coefficient polynomials, and use that theory to prove this statement.</p> <h2 id="proof-idea">Proof Idea</h2> <p>First we’ll give a high-level overview of the proof. We’ll start by factoring our polynomial $P$ as \[P(x) = c\cdot Q_1(x)^{e_1}\cdot Q_2(x)^{e_2}\cdots Q_k(x)^{e_k}\] for some distinct irreducible polynomials $Q_i(x) \in \mathbb{Z}[x]$ and some integer $c$. (Unique factorization <em>does</em> hold in $\mathbb{Z}[x]$, but if you are uncomfortable with this, then you can perform the factorization in $\mathbb{Q}[x]$ and clear denominators, allowing $c$ to be rational — this doesn’t affect the proof at all.)</p> <p>The main idea of the proof is to construct distinct primes $p_1$, $p_2$, $\ldots$, $p_k$ and some integer $n$, such that for each $i$, we have $\nu_{p_i}(Q_i(n)) = 1$ and $\nu_{p_i}(Q_j(n)) = 0$ for $j \neq i$ — we’re constructing one prime $p_i$ for each factor $Q_i$, and trying to use these primes to ‘pull out’ the exponents $e_i$. (We also want our primes $p_i$ to not divide $c$.) If we can do so, then we have $\nu_{p_i}(P(n)) = e_i$ for each $i$. Since $P(n)$ is the $d$th power of an integer for some $d &gt; 1$, then we must have $\gcd(e_1, \ldots, e_k) = d$, and therefore we can conclude that $P$ is the $d$th power of a polynomial.</p> <p>We’ll now see some facts about integer-coefficient polynomials that allow us to find such primes $p_1$, $\ldots$, $p_k$.</p> <h1 id="bounded-gcds">Bounded GCDs</h1> <p>In this section, we’ll prove the following lemma.</p> <div class="lemma"> Let $P, Q \in \mathbb{Z}[x]$ be two relatively prime polynomials. Then there is some constant $c$ such that $\gcd(P(n), Q(n)) \leq c$ for all integers $n$. </div> <p>In order to prove this, we can use the Euclidean Algorithm.</p> <h2 id="euclidean-algorithm-for-polynomials">Euclidean Algorithm for Polynomials</h2> <p>If we’re trying to find the gcd of two <em>integers</em> $a$ and $b$, we know that $\gcd(a, b) = \gcd(a - kb, b)$ for any integer $k$. So then we can repeatedly replace the larger number with its remainder mod the smaller number until we end up with $\gcd(0, d) = d$ for some nonzero integer $d$. For example, \[\gcd(20, 14) = \gcd(6, 14) = \gcd(6, 2) = \gcd(0, 2) = 2.\]</p> <p>We can do the same thing with polynomials with <em>rational</em> coefficients — given any two polynomials with rational coefficients, in order to find their gcd (the gcd of two rational-coefficient polynomials is defined as the highest-degree monic polynomial (also with rational coefficients) which divides both of them in $\mathbb{Q}[x]$), we can repeatedly replace the higher-degree one with its remainder mod the lower-degree one, until we end up with $\gcd(0, R) = R$ for some polynomial $R \in \mathbb{Q}[x]$. (The reason we’re using <em>rational</em> coefficients rather than integer coefficients here is because we can’t necessarily do polynomial division in $\mathbb{Z}[x]$ — if we’re dividing by a non-monic polynomial, we may introduce fractions.)</p> <div class="example"> Find $\gcd(x^3 + 3x + 1, x^2 - 2)$. </div> <div class="proof"> First, we can use polynomial division to get \[x^3 + 3x + 1 = (x^2 - 2)x + 5x + 1,\] which means \[\gcd(x^3 + 3x + 1, x^2 - 2) = \gcd(5x + 1, x^2 - 2).\] Now we can perform polynomial division again to get \[x^2 - 2 = (5x + 1)(\tfrac{1}{5}x - \tfrac{1}{25}) - \tfrac{49}{25}.\] So we have \[\gcd(5x + 1, x^2 - 2) = \gcd(5x + 1, \tfrac{49}{25}) = \gcd(0, \tfrac{49}{25}) = 1.\] </div> <p>This process successfully terminates (meaning that we can make one term $0$) for the same reason that it does in the integers — every step decreases the degree of our polynomials.</p> <p>Similarly to the case of integers, every polynomial we write down in this process is a linear combination of $P$ and $Q$ (where the coefficients are rational-coefficient polynomials). In the integer case, this gives us Bezout’s Theorem; similarly, here this means \[\gcd(P, Q) = A\cdot P + B\cdot Q\] for some rational-coefficient polynomials $A$ and $B$.</p> <h2 id="proving-the-lemma">Proving the Lemma</h2> <p>Now this gives us the tools to prove our lemma that relatively prime polynomials have bounded gcds.</p> <div class="proof"> Since $P$ and $Q$ are relatively prime, we have $\gcd(P, Q) = 1$, which means \[1 = A\cdot P + B\cdot Q\] for rational-coefficient polynomials $A$ and $B$. This means \[d = A^*\cdot P + B^* \cdot Q\] for <em>integer</em>-coefficient polynomials $A^*$ and $B^*$, by clearing denominators. So then $\gcd(P(n), Q(n))$ must divide $d$, for all integers $n$. </div> <h1 id="schurs-theorem">Schur’s Theorem</h1> <p>Earlier, we wrote our polynomial as \[P(x) = c\cdot Q_1(x)^{e_1}\cdot Q_2(x)^{e_2}\cdots Q_k(x)^{e_k},\] and we hoped to find primes $p_1$, $p_2$, $\ldots$, $p_k$ such that each $p_i$ functions as a sort of indicator for $Q_i(n)$ — we want $p_i$ to divide $Q_i(n)$, but not $Q_j(n)$ for any $j \neq i$.</p> <p>Since all the $Q_i$ are relatively prime as polynomials, we know that their pairwise gcds are all bounded. So if we choose our primes to be large enough, then if $p_i$ divides $Q_i(n)$ it <em>definitely</em> can’t divide $Q_j(n)$ for any other $j$.</p> <p>But what if we <em>can’t</em> choose a large enough prime — what if all primes which divided $Q_i(n)$ (over all $n$) were less than $100$, for instance? This is where Schur’s Theorem comes in.</p> <div class="theorem" text="Schur&apos;s Theorem"> If $P \in \mathbb{Z}[x]$ is a nonconstant polynomial, then there are infinitely many primes which divide $P(n)$ for some integer $n$. </div> <div class="proof"> Assume for contradiction that the only primes dividing $P(n)$, over all $n$, are $p_1$, $p_2$, $\ldots$, $p_r$. Now the idea is to construct a bunch of values of $n$ which have the same value of $P(n)$, by imposing conditions mod powers of these primes. <br/><br/> Fix some $a$ for which $P(a) \neq 0$, and for each prime $p_i$, let $\nu_{p_i}(P(a)) = e_i$. Then construct \[n \equiv a \pmod{p_i^{e_i + 1}}\] for all $i$ (this is possible by the Chinese Remainder Theorem). Then we have \[P(n) \equiv P(a) \pmod{p_i^{e_i + 1}}\] for all $i$ as well, which means \[\nu_{p_i}(P(n)) = \nu_{p_i}(P(a))\] for all $i$. But since these are the only primes which can divide either $P(n)$ or $P(a)$, this means we must have $|P(n)| = |P(a)|$. <br/><br/> So then there is some value which $P$ takes infinitely many times &mdash; namely, either $P(a)$ or $-P(a)$ &mdash; which means $P$ must be constant. </div> <p>So by Schur’s Theorem, we know that we can find arbitrarily large (and distinct) primes $p_1$, $p_2$, $\ldots$, $p_k$, and positive integers $n_1$, $n_2$, $\ldots$, $n_k$, such that $p_i \mid Q_i(n_i)$ for each $i$. We can then combine these $n_i$ into one $n$ by choosing \[n \equiv n_i \pmod{p_i}\] for all $i$ (which is possible by the Chinese Remainder Theorem). Then by the fact that gcds are bounded, we know that $p_i$ doesn’t divide $Q_j(n)$ for any $j \neq i$.</p> <h1 id="hensels-lemma">Hensel’s Lemma</h1> <p>We’ve <em>almost</em> constructed everything we wanted to — we now have our indicator primes, and we’ve shown that they aren’t affected by any factor except the one they’re assigned to. But there’s one thing missing. It’s not enough to know that $p_i \mid Q_i(n)$ — we need to know that <em>exactly one</em> power of $p_i$ divides $Q_i(n)$, or in other words, $\nu_{p_i}(Q_i(n)) = 1$. (This is so that we can get that the power of $p_i$ in the prime factorization of $P(n)$ is exactly $e_i$.)</p> <p>So we want to try actually choosing the $n_i$ mod $p_i^2$ such that $Q_i(n_i)$ is divisible by $p_i$, but not $p_i^2$. In order to do this, we’ll use Hensel’s Lemma:</p> <div class="theorem" desc="Hensel&apos;s Lemma"> Let $P \in \mathbb{Z}[x]$ be a polynomial, and $p$ a prime and $n_1$ an integer. If $p \nmid P'(n_1)$, then for any positive integer $e$ and any $c \equiv P(n_1) \pmod{p}$, we can find some $n_e \equiv n_1 \pmod{p}$ such that \[P(n_e) \equiv c \pmod{p^e}.\] </div> <p>So essentially, Hensel’s Lemma states that if we can solve a polynomial equation mod $p$, and at that point the <em>derivative</em> isn’t divisible by $p$, then we can solve it mod any power of $p$.</p> <p>Before we see the proof, we’ll first look at a specific example:</p> <div class="example"> Show that if $p &gt; 2$ and there exists $n_1$ such that $n_1^2 + 2 \equiv 0 \pmod{p}$, then for every integer $a$, there exists $n \equiv n_1 \pmod{p}$ such that $n^2 + 2 \equiv ap \pmod{p^2}$. </div> <div class="proof"> Let $n = n_1 + pt$, for some integer $t$. Then we have \[n^2 + 2 = n_1^2 + 2pn_1t + p^2t^2 + 2 \equiv n_1^2 + 2 + 2pn_1t \pmod{p^2}.\] We know $n_1^2 + 2$ is some multiple of $p$, and since $p \nmid 2n_1$, then $2pn_1t$ must run over all possible multiples of $p$. So as $t$ varies, $n^2 + 2$ must cover all multiples of $p$ mod $p^2$. </div> <p>The proof in the general case is essentially the same:</p> <div class="proof" text="Proof of Hensel&apos;s Lemma"> Induct on $e$ &mdash; in the base case $e = 1$, there is nothing to prove. Now choose $n_{e - 1} \equiv n_1 \pmod{p}$ such that $P(n_{e - 1}) \equiv c \pmod{p^{e - 1}}$, using the inductive hypothesis. Now let $n_e = n_{e - 1} + p^{e - 1}t$ for any integer $t$. <br/><br/> Let $P(x) = a_nx^n + a_{n - 1}x^{n - 1} + \cdots + a_1x + a_0$. Then we have \[(n_{e - 1} + p^{e - 1}t)^i \equiv n_{e - 1}^i + ip^{e - 1}tn_{e - 1}^{i - 1} \pmod{p^e}\] by the Binomial Theorem (every other term is divisible by $p^e$, since $2(e - 1) \geq e$). So this means \[P(n_e) \equiv P(n_{e - 1}) + p^{e - 1}\cdot P'(n_{e - 1})t \pmod{p^e}.\] Since $P'(n_{e - 1})$ is not divisible by $p$, then the second term must cover all multiples of $p^{e - 1}$ mod $p^e$, which means over all values of $t$, we must be able to get $P(n_e) \equiv c \pmod{p^e}$. </div> <p>Now in our case, we know $Q_i$ and $Q_i’$ are relatively prime, as polynomials (since the $Q_i$ are irreducible), so their gcds are again bounded. So as long as we chose our primes $p_i$ to be large enough, we know that if $p_i \mid Q_i(n_i)$, then $p_i \nmid Q_i’(n_i)$. So we can actually choose $n_i$ mod $p_i^2$ such that $Q_i(n_i) \equiv p_i \pmod{p_i^2}$.</p> <p>Then, by the Chinese Remainder Theorem we can again choose $n$ such that $n \equiv n_i \pmod{p_i^2}$ for all $i$. So for this choice of $n$, we have $\nu_{p_i}(Q_i(n)) = 1$ for all $i$.</p> <h1 id="conclusion">Conclusion</h1> <p>We’ve written \[P(x) = cQ_1(x)^{e_1}Q_2(x)^{e_2}\cdots Q_k(x)^{e_k}\] for irreducible polynomials $Q_i$, and chosen huge distinct primes $p_1$, $p_2$, $\ldots$, $p_k$ and a positive integer $n$ such that $\nu_{p_i}(Q_i(n)) = 1$ and $\nu_{p_i}(Q_j(n)) = 0$ for all $j \neq i$.</p> <p>This means for each $i$, we have \[\nu_{p_i}(P(n)) = e_i.\] But we know $P(n)$ is a $d$th power of an integer for some $d &gt; 1$. So $d$ must divide $e_i$ for all $i$. Finally, then $Q_1(n)^{e_1}\cdots Q_k(n)^{e_k}$ is a $d$th power, so $c$ must be a $d$th power as well.</p> <p>So $P$ is a $d$th power of a polynomial, and is therefore a perfect power.</p>]]></content><author><name></name></author><category term="number-theory"/><category term="olympiad"/><summary type="html"><![CDATA[Which integer-coefficient polynomials only attain values which are perfect powers?]]></summary></entry><entry><title type="html">Tournament of Towns</title><link href="https://sanjanad1024.github.io/blog/2022/tournament-of-towns/" rel="alternate" type="text/html" title="Tournament of Towns"/><published>2022-02-14T00:00:00+00:00</published><updated>2022-02-14T00:00:00+00:00</updated><id>https://sanjanad1024.github.io/blog/2022/tournament-of-towns</id><content type="html" xml:base="https://sanjanad1024.github.io/blog/2022/tournament-of-towns/"><![CDATA[<p>This winter, I looked at several problems from the <a href="https://www.turgor.ru/en/problems/">Tournament of Towns</a>; here are some that I especially liked. (The last two are among my favorite problems of all time.)</p> <div class="problem" text="ToT Fall 2009 S-A4"> Let $[n]!$ denote the product $1 \times 11 \times 111 \times \cdots \times (11\cdots 1)$ (where the last term has $n$ digits). Prove that $[n + m]!$ is divisible by $[n]!\times[m]!$. </div> <div class="sd-hidden" desc="Solution"> We have $[n]! = \frac{10 - 1}{9} \cdot \frac{10^2 - 1}{9} \cdots \frac{10^n - 1}{9}$, so it suffices to show \[\prod_{i = 1}^n (10^i - 1) \cdot \prod_{i = 1}^m (10^i - 1) \mid \prod_{i = 1}^{m + n} (10^i - 1).\] We'll show that for all primes $p$, the $\nu_p$ of the RHS is at least $\nu_p$ of the LHS. <br/><br/> Suppose $p$ is relatively prime to $10$ (otherwise $p$ cannot divide either side), and let $d = \operatorname{ord}_p 10$. Then only the terms with $i \mid d$ are relevant. If $ad$ and $bd$ are the greatest multiples of $d$ at most $n$ and $m$, respectively, then $(a + b)d \leq n + m$, so it suffices to show \[\sum_{i = 1}^a \nu_p(10^{id} - 1) + \sum_{i = 1}^b \nu_p(10^{id} - 1) \leq \sum_{i = 1}^{a + b} \nu_p(10^{id} - 1).\] But we have \[\nu_p(10^{id} - 1) = \nu_p(10^d - 1) + \nu_p(id) = \nu_p(10^d - 1) + \nu_p(i)\] by LTE. So it suffices to show \[\sum_{i = 1}^a \nu_p(i) + \sum_{i = 1}^b \nu_p(i) \leq \sum_{i = 1}^{a + b} \nu_p(i),\] which is true as $\binom{a + b}{a}$ is an integer. </div> <div class="problem" text="ToT Fall 2011 S-A6"> Prove that for $n \geq 2$, the integer \[1^1 + 3^3 + 5^5 + \cdots + (2^n - 1)^{2^n - 1}\] is a multiple of $2^n$ but not a multiple of $2^{n + 1}$. </div> <div class="sd-hidden" desc="Solution"> Induct on $n$. In the base case $n = 2$, $1^1 + 3^3 = 28$ is a multiple of $4$ but not $8$. <br/><br/> Now suppose $n \geq 3$ and assume this is true for $n - 1$, so \[1^1 + 3^3 + \cdots + (2^{n - 1} - 1)^{2^{n - 1} - 1} \equiv 2^{n - 1} \pmod{2^n}.\] Now note that for all odd $x$, we have \[\nu_2(x^{2^{n - 1}} - 1) = \nu_2(x^2 - 1) + \nu_2(2^{n - 2}) \geq n + 1,\] so then $x^{2^{n - 1}}$ is always $1$ mod $2^{n + 1}$. So then \[(x + 2^{n - 1})^{x + 2^{n - 1}} \equiv (x + 2^{n - 1})^x \equiv x^x + x^x \cdot 2^{n - 1},\] since $2(n - 1) \geq n + 1$. So this means \[1^1 + 3^3 + \cdots + (2^n - 1)^{2^n - 1} \equiv (2 + 2^{n - 1})(1^1 + 3^3 + \cdots + (2^{n - 1} - 1)^{2^{n - 1} - 1}) \pmod{2^{n + 1}}.\] But $2 + 2^{n - 1}$ is even, and the second sum is $2^{n - 1}$ mod $2^n$ by the induction hypothesis, so then the original sum is $2^n$ mod $2^{n + 1}$, as desired. </div> <div class="sd-hidden" desc="Comments"> It makes sense to induct because the sum is pretty intractable on its own, but when you lift from $2^n$ to $2^{n + 1}$, a lot of the terms are either the same or closely related. </div> <div class="problem" text="ToT Fall 2019 S-A7"> Some of the integers $1$, $2$, $\ldots$, $n$ have been colored red so that for each triplet of red numbers $a$, $b$, $c$ (not necessarily distinct), if $a(b - c)$ is a multiple of $n$ then $b = c$. Prove that there are no more than $\varphi(n)$ red numbers. </div> <div class="sd-hidden" desc="Solution"> Let $p_1 &lt; p_2 &lt; \cdots &lt; p_r$ be the primes dividing $n$ which divide some red number, and $q_1$, $q_2$, $\ldots$, $q_s$ the primes dividing $n$ which don't divide any red number. If $r = 0$ then all red numbers are relatively prime to $n$ so there are at most $\varphi(n)$ red numbers; now assume $r \geq 1$. <div class="claim-un"> There are at most $\frac{n}{p_r} \cdot \prod \frac{q_i - 1}{q_i}$ red numbers. </div> <div class="proof"> Let $n = p_r\cdot m$, so $m$ is divisible by each of the $q_i$. Then each residue class mod $m$ contains at most one red number (if $b \equiv c \pmod{m}$, then take $a$ to be a red multiple of $p_r$, so $a(b - c)$ is a multiple of $n$). Additionally, all red numbers are relatively prime to each $q_i$, so all red residues mod $m$ are relatively prime to each $q_i$ as well. So then there are at most $m \prod \frac{q_i - 1}{q_i}$ red numbers. </div> But we have \[\varphi(n) = n \cdot \prod_{i = 1}^r \frac{p_i - 1}{p_i} \cdot \prod_{i = 1}^s \frac{q_i - 1}{q_i}.\] We have the bound \[\prod_{i = 1}^r \frac{p_i - 1}{p_i} \geq \prod_{i = 2}^{p_i} \frac{i - 1}{i} = \frac{1}{p_i},\] so then our bound on the count of red numbers is at most $\varphi(n)$. </div> <div class="sd-hidden" desc="Comments"> You can start by trying small cases &mdash; here the small case is when we only have one prime dividing $n$ with a red multiple, since having none gives exactly $\varphi(n)$. If we look at which residues are allowed then we get exactly $\frac{n}{p}\prod \frac{q - 1}{q}$. Then if we try to go to a more general case, we realize that there <em>isn't</em> really a much better bound we can get (at least, not one that I could find), so we probably still have to use this bound &mdash; and then thinking about size a bit finishes. </div> <div class="problem" text="ToT Spring 2009 S-A7"> Initially, the number $6$ is written on a blackboard. On the $n$th step (for $n \geq 1$), if the number $k$ is on the blackboard, it is replaced with $k + \gcd(k, n)$. Prove that at each step, the number on the blackboard increases either by $1$ or by a prime number. </div> <div class="sd-hidden" desc="Solution"> The first step is $6 \to 7$, the second step is $7 \to 8$, and the third step is $8 \to 9$. <div class="claim-un"> Suppose that after step $n$, the number on the blackboard is $3n$. Then if the number next increases by more than $1$ on step $m$, it increases by a prime and becomes $3m$. </div> <div class="proof"> We have that $m$ is the smallest integer greater than $n$ for which \[\gcd(m, 3n + m - n - 1) = \gcd(m, 2n - 1) &gt; 1.\] But if $p$ is any prime dividing $2n - 1$, then $n \equiv \frac{p + 1}{2} \pmod{p}$, so the smallest $m &gt; n$ with $p \mid m$ is \[m = n + \frac{p - 1}{2}.\] So then the first $m$ for which the gcd is not $1$ is $m = n + \frac{p - 1}{2}$ where $p$ is the smallest prime dividing $2n - 1$, and here the gcd is \[\gcd(2n + p - 1, 2n - 1) = p.\] So step $m$ is \[3n + \frac{p - 1}{2} - 1 \to 3n + \frac{p - 1}{2} - 1 + p = 3m,\] as desired. </div> Since after step $3$ the number is $3\cdot 3$, by induction this means all additions are $1$ or prime, and each prime addition results in $3m$ on the board after turn $m$. </div> <div class="sd-hidden" desc="Comments"> I think this is a pretty rigid problem &mdash; the idea is to try out the process for small values of $n$, and then you notice that there's a pattern: every nontrivial jump ends up in a position $(n, 3n)$, which is surprising but turns out to be not that hard to prove. Maybe it is even expected that the sequence should have some nice property like this, because if there's no good relationship between $n$ and $k$, the problem seems pretty intractable. But even without that heuristic, doing small cases is a good idea. </div> <div class="problem" text="ToT Spring 2011 S-A6"> In every cell of a square table is a number. The sum of the largest two numbers in each row is $a$ and the sum of the largest two numbers in each column is $b$. Prove that $a = b$. </div> <div class="sd-hidden" desc="Solution"> Assume not, so WLOG $a &lt; b$. Label the rows and columns $1$ through $n$. Draw a graph on $n$ vertices and for each column, draw an edge between the row numbers with the two greatest elements of that column (breaking ties arbitrarily) &mdash; multiple edges between two vertices are allowed. <br/><br/> This graph has $n$ vertices and $n$ edges, so it must contain a cycle $(r_1r_2\cdots r_k)$, possibly of length $2$. Suppose edge $r_ir_{i + 1}$ corresponds to column $c_i$. Then if $x(r, c)$ denotes the entry in $(r, c)$, we have \[\sum x(r_i, c_i) + x(r_{i + 1}, c_i) = bn\] by looking at columns. But \[\sum x(r_i, c_{i - 1}) + x(r_i, c_i) \leq an\] by looking at rows (since each term is at most the sum of the two largest numbers in each row). But these are the same sum, so $bn \leq an$, contradiction as $a &lt; b$. </div> <div class="sd-hidden" desc="Comments"> I think this is an arrows problem &mdash; the idea is that you draw a vertical line for each column connecting the two largest numbers in that column, and then you can get a cycle by adding in horizontal lines, which are at most the largest numbers in the rows. <br/><br/> <center><img src="/assets/img/11ttssa6-arrows.png" width="250" height="auto"/></center> </div> <div class="problem" text="ToT Spring 2016 S-A4"> There are $64$ towns in a country, and some pairs of towns are connected by roads but we don't know these pairs. We may choose any pair of towns and find out whether they are connected by a road. Our aim is to determine whether it is possible to travel between any two towns using roads. Prove that there is no algorithm which would enable us to do this in less than $2016$ questions. </div> <div class="sd-hidden" desc="Solution"> Call the person answering us the <em>oracle</em>; we'll show that the oracle can ensure that at every step until the end, the current knowledge is compatible with both the graph being connected and not. <br/><br/> Color an edge red if the oracle answers no, and blue if yes. Define a <em>blob</em> to be a set of vertices $S$ such that all edges in $S$ are drawn, and the blue edges form exactly a tree. Call a position <em>great</em> if it is a collection of blobs (possibly with size 1) so that there are no blue edges between distinct blobs. <br/><br/> Then the oracle can preserve greatness: suppose the position is great, and we query $uv$ with $u$ in blob $S$ and $v$ in blob $T$. The oracle answers no unless every other edge between blobs $S$ and $T$ has been queried (and colored red), in which case he answers yes. <br/><br/> This preserves greatness, as an answer of yes merges the blobs into a bigger blob. The graph will end up connected, but in the last turn there were two blobs, and if the oracle had answered no instead then the graph would be disconnected. So this works. </div> <div class="sd-hidden" desc="Comments"> This is essentially the strategy of saying yes unless forced to say no &mdash; if you have a connected blue subgraph and the other edges between vertices in that subgraph have not been colored red, then you don't ever have to query those edges. Any strategy that preserves greatness and ends with the graph connected should work; in particular saying no unless that would disconnect the graph works as well. </div> <div class="problem" text="ToT Spring 2021 J-A7"> Let $p$ and $q$ be two coprime positive integers. A frog hops along the number line such that on each hop, it moves either $p$ units to the right or $q$ units to the left. Eventually, the frog returns to the initial point. Prove that for every positive integer $d &lt; p + q$, there are two numbers visited by the frog which differ by $d$. </div> <div class="sd-hidden" desc="Solution"> Write the list of jumps $+p$ and $-q$ made by the frog in a circle, so there are $kq$ points on the circle labelled $+p$ and $kp$ points labelled $-q$, in some order. Then it suffices to show that there is some subset of consecutive points on this circle whose sum of labels is exactly $d$. <br/><br/> By Bezout's Theorem there are positive integers $a$ and $b$ with $d = ap - bq$. Look at subsets of exactly $a + b$ consecutive points on the circle. Then it suffices to show there is a subset with at most $a$ points labelled $+p$, and a subset with at least $a$ points labelled $+p$ &mdash; as we walk around the circle (taking the subset starting at each point), the number of points $+p$ in the subset changes by $0$ or $\pm 1$ each step, so by Discrete IVT there then must exist a subset with exactly $a$ points $+p$. <br/><br/> First assume for contradiction that all subsets have at least $a + 1$ points labelled $+p$. Then sum over all $k(p + q)$ subsets. Each point is counted in $a + b$ subsets, and there are exactly $kq$ points $+p$, so then \[k(p + q)(a + 1) \leq kq(a + b).\] This implies $ap - bq \leq -p - q$, contradiction. Similarly, if all subsets have at most $a - 1$ points $+p$, then \[k(p + q)(a - 1) \geq kq(a + b),\] which implies $ap - bq \geq p + q$, contradiction. <br/><br/> So then there exists a subset with at most $a$ points $+p$, and a subset with at least $a$ points $+p$, so by Discrete IVT there exists one with exactly $a$ points $+p$, and this subset sums to exactly $d$. </div> <div class="sd-hidden" desc="Comments"> I really like this problem; I think it's a cool combination of local (look at a frame and shift it) and global (sum over all frames) ideas. I think the main realization is that you want to write the jumps in a circle and look at a <em>fixed</em> set of counts (meaning a specific pair of $a$ and $b$) &mdash; you can sort of motivate this by the fact that you want to look at <em>something</em>, and keeping track of multiple possible solutions would be hard as these seem pretty unrelated to each other (and there sometimes is only one). </div> <div class="problem" text="ToT Spring 2019 S-A7"> Consider lattice paths of finite length which start from $(0, 0)$ and move only up and right, which we call <em>skeletons</em>. For each skeleton, we define a <em>worm</em> consisting of all unit cells in the plane sharing at least one point with the skeleton. (For example, for the path from $(0, 0)$ to $(1, 0)$, the corresponding worm has six cells). Prove that for every integer $n &gt; 2$, the number of worms which can be tiled by dominoes in exactly $n$ ways is equal to $\varphi(n)$. </div> <div class="sd-hidden" desc="Solution"> First, we're going to find a way to recurse the number of tilings of a worm. <br/><br/> Note that the number of ways to tile the worm of skeleton $(0, 0)$ is $2$, since it is a $2 \times 2$ grid. Also, define the number of ways to tile the worm of the empty skeleton as $1$. <br/><br/> Define the <em>twisty part</em> of a skeleton $S$ to be the maximal sequence of points starting with the end for which the skeleton alternates direction, and define the <em>head</em> of a skeleton to be the remainder not in the twisty part (the head may be empty). Here are some examples, with the heads in red and the twisty parts in blue. <br/><br/> <center><img src="/assets/img/19ttssa7-twistypart.png" width="400" height="auto"/></center> <div class="claim-un"> To get the number of ways to tile skeleton $S$, we add the number of ways to tile the skeleton with the last point removed, and the number of ways to tile the head of $S$. </div> <div class="proof"> WLOG the last move in the skeleton is vertical. Now if we place a horizontal domino at the top-right corner, the remaining worm is the worm of the skeleton where we delete the last point of $S$. <br/><br/> <center><img src="/assets/img/19ttssa7-recurse1.png" width="250" height="auto"/></center> Meanwhile, if we place a vertical domino in the top-right corner, then this forces the positions of a bunch of other dominoes. Placing all these other dominoes has the effect of removing the entire twisty part, leaving us with the worm of the head of $S$. <br/><br/> <center><img src="/assets/img/19ttssa7-recurse2.png" width="250" height="auto"/></center> So then the total number of ways to tile $S$ is the sum of these two new skeletons, the one with the last point deleted and the head. </div> Now we can make a big tree, where the top entry is the skeleton $(0, 0)$, and when we go down, left means adding a step right while right means adding a step up. At each node, we write the number of ways to tile that skeleton. We also write the two entries we summed according to the previous claim &mdash; if this was a left child, then we write the skeleton with the last element removed on the left and the head on the right, while if this was a right child then we do the opposite. <br/><br/> <center><img src="/assets/img/19ttssa7-tree.png" width="600" height="auto"/></center> We claim this becomes the Euclidean Algorithm tree where $(a, b)$ has descendants $(a + b, a)$ and $(b, a + b)$. <br/><br/> To show this, if $(a, b)$ with skeleton $S$ is a left child, then $a$ is the number of ways for $S$ with its last point removed, and $b$ is the number of ways for the head of $S$. <br/><br/> Then if we go left one step to get skeleton $T$, then $S$ with $a + b$ ways is $T$ with its last point removed, and $S$ with its last point removed is the head of $T$ (since the twisty part is only the last two points), which gives $(a + b, a)$. <br/><br/> Meanwhile, if we go one step right to get skeleton $T$, then $S$ is still $T$ with its last point removed, but the head of $S$ is the head of $T$ (since we went left to get to $S$, and then right to get to $T$, so $T$ swallows the twisty part of $S$), which gives us $(b, a + b)$. <br/><br/> Similar things happen when $S$ is a right child, which proves that our tree is the Euclidean Algorithm tree. But it's clear that every pair $(a, b)$ with $\gcd(a, b) = 1$ occurs exactly once in this tree (since we can trace it back to $(1, 1)$ uniquely), and no pairs with $\gcd(a, b) &gt; 1$ are in this tree. So then the number of occurrences of $n$ in this tree are the number of ways to write $n = a + b$ with $\gcd(a, b) = 1$, which is $\varphi(n)$. </div> <div class="sd-hidden" desc="Comments"> This is one of my favorite problems of all time. It's a rigid problem &mdash; the point is to start by trying to figure out how to count the number of tilings of a given worm. You can try placing the tile at the end and drawing the dominoes that are forced, and you notice that you end up with a smaller skeleton with a nice characterization. Once you get the recursion you can try drawing a tree to perform the recursion for small worms, and eventually you notice that this is very similar to the Euclidean Algorithm tree &mdash; and you can do the tracking in such a way that it actually <em>becomes</em> the Euclidean Algorithm tree. </div>]]></content><author><name></name></author><category term="olympiad"/><summary type="html"><![CDATA[Some cool problems from the Tournament of Towns.]]></summary></entry></feed>