<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://sanjanad1024.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sanjanad1024.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-03T23:22:26+00:00</updated><id>https://sanjanad1024.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal page of Sanjana Das. </subtitle><entry><title type="html">Setup of interactive proofs</title><link href="https://sanjanad1024.github.io/blog/2024/interactive-proofs/" rel="alternate" type="text/html" title="Setup of interactive proofs"/><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://sanjanad1024.github.io/blog/2024/interactive-proofs</id><content type="html" xml:base="https://sanjanad1024.github.io/blog/2024/interactive-proofs/"><![CDATA[<p>In this post, I’ll give an informal description of what interactive proofs are, and a few simple examples. The content of this post is based on lectures from the classes 18.404 (the lecture from December 7) and 18.405 (the lecture from April 11) at MIT. (This post is primarily setup for a few future posts on some really cool things we can do with interactive proofs.)</p> <p>We’ll first set up <em>deterministic</em> interactive proofs (to illustrate the model in a simpler setting, and to motivate what follows — in particular, why we need randomness); and then we’ll extend the model to <em>randomized</em> interactive proofs (which are what we’re really interested in).</p> <h1 id="deterministic-interactive-proofs">Deterministic interactive proofs</h1> <p>Imagine we’ve got a <span class="vocab">prover</span>, who we think of as all-powerful, and a <span class="vocab">verifier</span>, who we think of as computationally bounded. (This means we want the verifier to run in polynomial time; we don’t care how long the prover takes to run.)</p> <p>Imagine we’ve got a statement $x$, and the prover wants to convince the verifier that $x$ is true. To do so, the prover looks at $x$ and sends the verifier some message $m_1$. And then the verifier looks at $x$ and $m_1$, and sends the prover some message $m_2$. And then the prover looks at $x$, $m_1$, and $m_2$, and sends the prover some message $m_3$. And so on — the prover and verifier keep on talking back and forth, and eventually the verifier decides to <em>accept</em> (meaning they believe the prover and agree $x$ is true) or <em>reject</em> (meaning they don’t believe the prover and think $x$ is false).</p> <center><img src="/assets/img/ipsat1.png" width="400" height="auto"/></center> <p>Note that since the verifier is supposed to run in $\textsf{poly}(\lvert x\rvert)$ time, the number of messages and the length of each message should be $\textsf{poly}(\lvert x\rvert)$.</p> <div class="question"> What kinds of statements can the prover convince the verifier of? Or in other words, what kinds of problems can we 'solve' using such a proof system? </div> <p>To formalize this, we need to define what it means to solve a problem using such a proof system. Imagine we’ve got a decision problem, which we can think of as a function \(f \colon \{0, 1\}^\ast \to \{0, 1\}\) (where we encode the input $x$ as a bit-string, and $f(x)$ is $1$ if $x$ is a $\textsf{YES}$ instance to the decision problem and $0$ if $x$ is a $\textsf{NO}$ instance) — so here the statement the prover is trying to convince the verifier of is that $f(x) = 1$.</p> <p>What should it mean for an interactive proof to ‘solve’ $f$? When we talk about an interactive proof solving $f$, we’re going to say essentially that there is a <em>verifier</em> who can be convinced of true statements of the form $f(x) = 1$, but not false ones. So on one hand, whenever $f(x) = 1$, there should be a <span class="vocab">good prover</span> that successfully convinces the verifier that $f(x) = 1$ (because this is true). On the other hand, when $f(x) = 0$, no <span class="vocab">cheating prover</span> should be able to convince the verifier that $f(x) = 1$ (because this is false) — so no matter what the prover tries to do, the verifier should reject.</p> <div class="definition"> We say $f$ <span class="vocab">has a deterministic interactive proof</span> if there is a verifier such that: <ul> <li> <span class="vocab">(Correctness)</span> For each $x$ such that $f(x) = 1$, there exists a prover that makes the verifier accept. </li> <li> <span class="vocab">(Soundness)</span> For each $x$ such that $f(x) = 0$, there does <em>not</em> exist a prover that makes the verifier accept (i.e., <em>every</em> possible prover causes the verifier to reject). </li> </ul> </div> <p>In other words, true statements of the form $f(x) = 1$ should have good proofs (that the verifier accepts), and false statements should not.</p> <div class="definition"> We define $\textsf{DIP}$ as the class of decision problems which have a deterministic interactive proof. </div> <h2 id="an-example--textsfsat">An example — $\textsf{SAT}$</h2> <p>To illustrate this definition, here’s an example of a deterministic interactive proof for $\textsf{SAT}$ — the problem where we’re given a Boolean formula $\varphi$, and we want to figure out whether it’s satisfiable or not.</p> <div class="definition"> We define $\textsf{SAT}$ as the following decision problem: <ul> <li> <b>Input:</b> a Boolean formula $\varphi$ (in some collection of variables $x_1$, $\ldots$, $x_n$, and with the operations $\wedge$, $\vee$, and $\neg$, which denote $\textsf{AND}$, $\textsf{OR}$, and $\textsf{NOT}$). </li> <li> <b>Decide:</b> whether there exists a <span class="vocab">satisfying assignment</span> to $\varphi$ &mdash; i.e., an assignment of values $a_1$, $\ldots$, $a_n$ to the variables (with $a_i \in \{\texttt{T}, \texttt{F}\}$ for each $i$) such that plugging them in makes $\varphi$ evaluate to $\texttt{T}$. </li> </ul> </div> <p>(In the notation where we think of decision problems as functions \(\{0, 1\}^\ast \to \{0, 1\}\), we’d say that $\textsf{SAT}(\varphi)$ is $1$ if $\varphi$ is satisfiable and $0$ if not; we think of $\varphi$ as being encoded as a bit-string in some reasonable way.)</p> <div class="example"> The problem $\textsf{SAT}$ has a deterministic interactive proof, where the verifier and good prover (for $\textsf{YES}$ instances) are defined as follows. On input $\varphi$: <ul> <li> The good prover finds a satisfying assignment to $\varphi$ and sends it to the verifier. </li> <li> The verifier plugs the assignment it receives into $\varphi$ and checks that it really is a satisfying assignment (and <span class="vocab">accepts</span> if yes and <span class="vocab">rejects</span> if no). </li> </ul> </div> <p>Note that we don’t need to care how long it takes the <em>prover</em> to come up with the satisfying assignment; what’s important is that the <em>verifier</em> can check that it’s really a satisfying assignment in polynomial time.</p> <p>This protocol satisfies correctness — if $\varphi$ is satisfiable, then the good prover will make the verifier accept. It also satisfies soundness — if $\varphi$ is <em>not</em> satisfiable, then no matter what assignment the cheating prover sends, when the verifier plugs it in, they’ll find that it’s not a satisfying assignment (because <em>no</em> assignment is), and so they’ll reject.</p> <h2 id="textsfdip--textsfnp">$\textsf{DIP} = \textsf{NP}$</h2> <p>Note that the protocol we gave for $\textsf{SAT}$ doesn’t involve any interaction at all — the prover just sends one message, and the verifier just looks at it and decides to accept or reject. In fact, this type of protocol corresponds exactly to the class $\textsf{NP}$. To see this, one way of defining $\textsf{NP}$ is as the class of problems with <span class="vocab">efficiently verifiable certificates</span>.</p> <div class="definition"> We define $\textsf{NP}$ as the class of decision problems $f$ such that there exists a polynomial-time algorithm $\mathcal{V}$ such that for all $x$, we have \[f(x) = 1 \iff (\exists \, y \text{ of length } \textsf{poly}(\lvert x\rvert))[\mathcal{V}(x, y) \text{ accepts}].\] </div> <p>In words, we think of $\mathcal{V}$ as a <span class="vocab">$\textsf{NP}$-verifier</span> for $f$ — it’s an efficient algorithm that takes in both our input $x$ and a polynomial-length <span class="vocab">certificate</span> $y$, and checks that $y$ is a ‘good certificate’ for $x$. And $\textsf{YES}$ instances $x$ should have good certificates (i.e., when $f(x) = 1$ there should exist a good certificate $y$), while $\textsf{NO}$ instances should <em>not</em> have good certificates.</p> <div class="example"> We have $\textsf{SAT} \in \textsf{NP}$ &mdash; we can define a 'good certificate' for $\varphi$ to be a satisfying assignment to $\varphi$ (we can construct a $\textsf{NP}$-verifier $\mathcal{V}$ which checks whether some assignment is a good certificate for $\varphi$ by simply plugging it in; and by definition $\varphi$ is a $\textsf{YES}$ instance to $\textsf{SAT}$ if and only if there exists a satisfying assignment, i.e., a good certificate for $\varphi$). </div> <p>In this definition, we can think of $\textsf{NP}$ as the class of problems $f$ which have a deterministic interactive proof with no interaction — the prover just sends a single message, and the verifier decides to accept or reject. (As in <span class="crossref">Example 5</span>, we define the good prover to send over a good certificate for the input $x$, and the verifier to check that the certificate it received really works — in particular, the verifier for the interactive proof is exactly the $\textsf{NP}$-verifier $\mathcal{V}$.) In particular, this immediately means $\textsf{NP} \subseteq \textsf{DIP}$ (as $\textsf{NP}$ is a special case of $\textsf{DIP}$).</p> <p>Our definition of deterministic interactive proofs allows for interaction, so we might hope that it makes the proof system more powerful. Unfortunately, this is false — it turns out that even <em>with</em> interaction, we can’t use such protocols to solve any problems other than the ones already in $\textsf{NP}$ (which we could have solved even without interaction).</p> <div class="theorem"> We have $\textsf{DIP} = \textsf{NP}$. </div> <div class="proof"> We've already seen that $\textsf{NP} \subseteq \textsf{DIP}$, so it remains to show that $\textsf{DIP} \subseteq \textsf{NP}$. Suppose $f \in \textsf{DIP}$, so we've got some deterministic interactive proof for $f$ &mdash; this means the prover and verifier talk back and forth, and eventually the verifier decides to accept or reject. Our goal is to remove the interaction from this protocol &mdash; i.e., to turn it into one where the prover sends a single message, and the verifier just decides whether to accept or reject. (This is because non-interactive deterministic interactive proofs correspond exactly to $\textsf{NP}$, as seen above.) <br/><br/> The idea is that because the verifier is deterministic, given the input $x$, the prover can know exactly what the entire interaction will look like &mdash; they know they're going to send a message $m_1$; then they can look at $x$ and $m_1$ and figure out exactly what message $m_2$ the verifier is going to send; then they can look at $x$, $m_1$, and $m_2$ and figure out exactly what message $m_3$ they'd respond with; and so on. <br/><br/> And this means the prover can just send the verifier their <em>entire</em> end of the interaction all in one message &mdash; i.e., the prover sends the verifier $(m_1, m_3, \ldots)$. And the verifier then simulates the interaction by themselves, plugging in these messages for the prover's end of the interaction &mdash; they look at $x$, imagine that the prover sends them $m_1$ and figure out what message $m_2$ they'd respond with, imagine that the prover then sends them $m_3$ and figure out what message $m_4$ they'd respond with, and so on; and in the end, they see whether they'd accept or reject (and they make the same decision here). <br/><br/> If $f(x) = 1$, then there's some good prover for the original protocol that makes the original verifier accept; so if we take $(m_1, m_3, \ldots)$ to correspond to that good prover, then the new verifier will accept as well (since it's just simulating the original verifier's interaction with that good prover). Meanwhile, if $f(x) = 0$, then every cheating prover for the original protocol makes the original verifier reject, so no matter what list of messages $(m_1, m_3, \ldots)$ the new cheating prover sends, the new verifier will reject as well (since it'll be simulating the original verifier's interaction with some cheating prover). </div> <h1 id="randomized-interactive-proofs">Randomized interactive proofs</h1> <p>We’ve seen that with deterministic interactive proofs, we don’t get anything out of interaction — in other words, any deterministic interactive proof that involves interaction can be turned into one that doesn’t (which means $\textsf{DIP} = \textsf{NP}$). And the reason for this was essentially that if the verifier is deterministic, then the prover can predict in advance <em>everything</em> that the verifier is going to say, so they can just have all their responses ready and send them all at once.</p> <p>We don’t like this — our hope is to define a reasonable model of interactive proofs that <em>increases</em> the class of problems we could solve. And the way we’re going to do this is by allowing the verifier to be <em>randomized</em> — this means the verifier gets to toss some coins when it’s choosing its message. So the prover looks at the input $x$ and sends a message $m_1$, as before. And then the verifier looks at $x$ and $m_1$ <em>and tosses some coins</em> to come up with a message $m_2$ to respond with. And then the prover looks at $x$, $m_1$, and $m_2$ to come up with a message $m_3$, and the verifier looks at $x$, $m_1$, $m_2$, and $m_3$ and tosses some more coins to come up with a message $m_4$, and so on; and in the end the verifier either accepts or rejects.</p> <center><img src="/assets/img/ipsat2.png" width="400" height="auto"/></center> <p>(In this picture, $r_i$ denotes the randomness the verifier uses at each step — i.e., the outcomes of the $i$th set of coin tosses.)</p> <div class="remark"> We're using a <span class="vocab">private-coin model</span>, where the prover doesn't get to see the outcomes of the verifier's coin tosses. But you could also imagine a <span class="vocab">public-coin model</span>, where the prover <em>does</em> get to see these outcomes. Of course, any public-coin interactive proof can be made into a private-coin one (where we have the verifier simply announce the outcomes of their coin tosses together with the corresponding message). Quite surprisingly, it turns out that the converse is true as well &mdash; any private-coin protocol can also be made into a public-coin one! This is a very nice <a href="https://www.cs.toronto.edu/tss/files/papers/goldwasser-Sipser.pdf" target="_blank">result</a> of Goldwasser and Sipser (1986). <br/><br/> I'm probably not going to write more about the public-coin model; the protocol for graph non-isomorphism we'll see in this post will rely quite crucially on the fact that the prover doesn't see the verifier's coins (it can be converted to a public-coin protocol by the result of Goldwasser&ndash;Sipser, but this isn't obvious); but the protocols we'll see in future posts won't be (i.e., they can directly be viewed as public-coin protocols as well). </div> <p>We now need to define what it means for a randomized interactive proof to solve a decision problem $f$. This will be very similar to the definition for deterministic interactive proofs, but now that we’ve got randomness, we need to allow some probability of error (otherwise the randomness wouldn’t have any point).</p> <div class="definition"> We say $f$ <span class="vocab">has an interactive proof</span> if there is a verifier such that: <ul> <li> <span class="vocab">(Correctness)</span> For each $x$ such that $f(x) = 1$, there exists a prover that makes the verifier accept with probability at least $\frac{2}{3}$. </li> <li> <span class="vocab">(Soundness)</span> For each $x$ such that $f(x) = 0$, for every possible prover, the probability that the verifier accepts is at most $\frac{1}{3}$. </li> </ul> </div> <p>There’s a few decisions we’ve made with these definitions that aren’t obvious (i.e., it’d have been reasonable to do the opposite) but turn out not to matter:</p> <ul> <li> The values of $\frac{2}{3}$ (which we call the <span class="vocab">correctness parameter</span>) and $\frac{1}{3}$ (the <span class="vocab">soundness parameter</span>) are arbitrary &mdash; if we've got a protocol with these parameters, then we can get one with correctness parameter $1 - \frac{1}{2^k}$ and soundness parameter $\frac{1}{2^k}$ by simply running $\textsf{poly}(k)$ independent trials of the original one (and taking their majority outcome). </li> <li> We're allowing error in both the cases $f(x) = 0$ and $f(x) = 1$. But we could imagine instead allowing error in only one of the cases &mdash; we say a protocol has <span class="vocab">perfect correctness</span> if it only has error in the case $f(x) = 0$ (i.e., when $f(x) = 1$, there's a prover that makes the verifier <em>always</em> accept). <br/><br/> It turns out that any interactive proof protocol can be converted to one with perfect correctness; this is not obvious and is also a very nice result, though I'm not sure who it's due to. All the protocols we'll discuss will have perfect correctness. <br/><br/> (On the other hand, the error in soundness <em>is</em> necessary &mdash; it's <em>not</em> true that we can convert any protocol to one with perfect soundness.) </li> <li> We've made the <em>verifier</em> randomized, but we're still keeping the <em>prover</em> deterministic. We could imagine allowing the prover to be randomized as well, but this wouldn't increase the power of the system &mdash; the prover is computationally unbounded, so we can without loss of generality assume that the prover always (deterministically) chooses the response that maximizes the probability of the verifier accepting. </li> </ul> <h2 id="an-example--graph-non-isomorphism">An example — graph non-isomorphism</h2> <p>We first considered the model of <em>deterministic</em> interactive proofs, and we saw that the only problems we can solve using them are the problems in $\textsf{NP}$ (for which we don’t even need the interaction). So the first thing we might wonder is, does this new model of <em>randomized</em> interactive proofs, have the same issue, or does it actually allow us to solve new problems?</p> <p>So the first thing we’ll see is an example of an interactive proof for a problem that we don’t know to be in $\textsf{NP}$ — the problem of determining whether two graphs are <em>non</em>-isomorphic.</p> <div class="definition"> Two graphs $G_1$ and $G_2$ are <span class="vocab">isomorphic</span> if there's a way to permute the vertices of $G_1$ that turns it into $G_2$ &mdash; i.e., a permutation $\pi \colon V(G_1) \to V(G_2)$ such that $\{u, v\}$ is an edge in $G_1$ if and only if $\{\pi(u), \pi(v)\}$ is an edge in $G_2$. </div> <div class="definition"> We define $\textsf{GNI}$ as the following decision problem: <ul> <li> <b>Input:</b> two graphs $G_1$ and $G_2$ (with the same number of vertices). </li> <li> <b>Decide:</b> whether $G_1$ and $G_2$ are <em>non</em>-isomorphic. </li> </ul> </div> <p>Written as a function \(\{0, 1\}^\ast \to \{0, 1\}\), we’d say that $\textsf{GNI}(G_1, G_2)$ is $1$ if $G_1$ and $G_2$ are non-isomorphic, and $0$ if they are isomorphic.</p> <p>The opposite problem $\textsf{GI}$, of determining whether two given graphs $G_1$ and $G_2$ <em>are</em> isomorphic, is in $\textsf{NP}$ — we can take the certificate to simply be the permutation $\pi \colon V(G_1) \to V(G_2)$ that turns $G_1$ into $G_2$. This in particular means $\textsf{GI}$ has an interactive proof (that doesn’t involve any interaction), as with all problems in $\textsf{NP}$.</p> <p>But it <em>isn’t</em> clear whether $\textsf{GNI}$ is in $\textsf{NP}$ — there’s a simple certificate showing that two graphs <em>are</em> isomorphic, but we don’t know of a certificate showing that they’re <em>not</em> isomorphic.</p> <p>Still, it turns out that $\textsf{GNI}$ <em>does</em> have a (randomized) interactive proof!</p> <div class="example"> The problem $\textsf{GNI}$ has an interactive proof &mdash; on input $(G_1, G_2)$: <ul> <li> The verifier chooses $i \in \{1, 2\}$ uniformly at random and permutes the vertices of $G_i$ uniformly at random to produce a new graph $H$. </li> <li> The verifier sends the prover $H$ and asks them which of $G_1$ and $G_2$ it came from (i.e., what the value of $i$ is). They <span class="vocab">accept</span> if the prover answers correctly and <span class="vocab">reject</span> if not. </li> </ul> </div> <div class="proof"> If $G_1$ and $G_2$ are indeed non-isomorphic, then there's a good prover who'll always be able to answer correctly &mdash; $H$ must be isomorphic to the graph $G_i$ that the verifier chose (by construction), and it can't be isomorphic to the other one (because if it were isomorphic to both, then $G_1$ and $G_2$ are isomorphic). So the prover can simply recover $i$ by checking which of $G_1$ and $G_2$ it's isomorphic to. <br/><br/> Meanwhile, if $G_1$ and $G_2$ are isomorphic, then the graph $H$ that the prover sees has nothing to do with $i$ (i.e., it has the same distribution whether $i$ is $1$ or $2$ &mdash; either way, it's a uniform random element of their isomorphism class). So no matter what the cheating prover tries to do, they're essentially being asked to predict a random coin toss with no useful information, which means they'll only have a $\frac{1}{2}$ chance of being correct. </div> <h2 id="the-class-textsfip">The class $\textsf{IP}$</h2> <p>The example of an interactive proof for $\textsf{GNI}$ shows that adding randomness to our model has done <em>something</em> nontrivial — it’s allowed us to solve at least one problem that we wouldn’t have known how to solve without it. So that’s good news; but now we’d like to get a better understanding of <em>how</em> much more we can now solve.</p> <div class="definition"> We define $\textsf{IP}$ as the class of decision problems $f$ which have an interactive proof. </div> <div class="question"> How powerful is $\textsf{IP}$? </div> <p>It turns out, quite surprisingly, that $\textsf{IP}$ is <em>extremely</em> powerful!</p> <div class="theorem" text="Shamir 1990"> We have $\textsf{IP} = \textsf{PSPACE}$. </div> <p>(The class $\textsf{PSPACE}$ is defined as the class of problems which can be solved in polynomial <em>space</em>.)</p> <p>This is quite amazing — for example, it means that there’s an interactive proof that a Boolean formula is <em>not</em> satisfiable (which is already quite non-obvious), as well as lots of problems that look much harder.</p> <p>One direction of <span class="crossref">Theorem 16</span>, that $\textsf{IP} \subseteq \textsf{PSPACE}$, isn’t too difficult — the idea is that if we start with some interactive proof for some decision problem $f$, then by carefully going through all possible messages the prover and verifier could send, we can calculate the probability that the ‘optimal prover’ makes the verifier accept; and we can do this only using polynomial space. Here are the details of this argument.</p> <div class="lemma"> We have $\textsf{IP} \subseteq \textsf{PSPACE}$. </div> <div class="proof"> Suppose that $f \in \textsf{IP}$, so it has some verifier as in the definition of $\textsf{IP}$. Then in order to decide $f$ on an input $x$, our goal is to calculate the maximum (over all possible provers) of the probability (over the verifier's internal randomness) that the verifier accepts &mdash; because we know this probability is at least $\frac{2}{3}$ if $f(x) = 1$ (by correctness) and at most $\frac{1}{3}$ if $f(x) = 0$ (by soundness). <br/><br/> To do so, suppose the protocol runs for $k$ rounds (where $k = \textsf{poly}(\lvert x\rvert)$). Then for each $0 \leq i \leq k$, we'll define $\textsf{Prob}_i(x, m_1, \ldots, m_i)$ as the maximum (over all possible provers) probability that the verifier accepts if we start up the protocol from the end of the $i$th round pretending that these messages have already occurred (i.e., we imagine that the prover has already sent $m_1$, the verifier has already responded with $m_2$, and so on up to $m_i$; and now we're starting the protocol up from the end of the $i$th round, and we want to think about how it continues). Then our goal is to compute $\textsf{Prob}_0(x)$. <br/><br/> We'll compute these quantities recursively &mdash; suppose we're trying to compute $\textsf{Prob}_{i - 1}(x, m_1, \ldots, m_{i - 1})$. Then we consider what happens on the $i$th round. If $i$ is odd, meaning that the prover speaks, then since we're trying to take a <em>maximum</em> over all possible provers (i.e., we want to consider the <em>best</em> possible message $m_i$ for the prover to send), we have \[\textsf{Prob}_{i - 1}(x, m_1, \ldots, m_{i - 1}) = \max_{m_i}\textsf{Prob}_i(x, m_1, \ldots, m_i).\] Meanwhile, if $i$ is even, meaning that the verifier speaks, then we want to consider the <em>distribution</em> (over the verifier's internal randomness) of the message $m_i$ it sends (given the previous messages), and compute the appropriate weighted average of the acceptance probabilities of the remainder of the protocol, so \[\textsf{Prob}_{i - 1}(x, m_1, \ldots, m_{i - 1}) = \sum_i \mathbb{P}[\text{verifier sends $m_i$}]\cdot \textsf{Prob}_{i - 1}(x, m_1, \ldots, m_i).\] This gives a recursion that we can compute in polynomial space &mdash; we start with $i = 0$, then iterate over all messages $m_1$ one at a time, then iterate over all messages $m_2$ one at a time (for each $m_1$), and so on. At every layer of the recursion, we essentially only need to store the current message $m_i$ we're trying, as well as our current computation for the maximum or sum (note that the quantities $\mathbb{P}[\text{verifier sends $m_i$}]$ can be computed in polynomial space as well, simply by iterating over all possible outcomes of the verifier's coin tosses one at a time and running the verifier with each). <br/><br/> So we can use this recursion to compute $\textsf{Prob}_0(x)$ in polynomial space, which lets us solve $f$. </div> <p>So the direction that $\textsf{IP} \subseteq \textsf{PSPACE}$ is not too hard. On the other hand, the direction that $\textsf{PSPACE} \subseteq \textsf{IP}$ — that every problem which can be solved in polynomial <em>space</em> in fact has an interactive proof where the verifier runs in polynomial <em>time</em> — is much more surprising, and the proof involves several very cool ideas. I’ll explain the proof in future posts.</p>]]></content><author><name></name></author><category term="complexity,"/><category term="TCS"/><summary type="html"><![CDATA[An informal description of how interactive proofs work and a few examples.]]></summary></entry><entry><title type="html">Adjoint of a Compact Operator</title><link href="https://sanjanad1024.github.io/blog/2024/adjoint-compact/" rel="alternate" type="text/html" title="Adjoint of a Compact Operator"/><published>2024-06-02T00:00:00+00:00</published><updated>2024-06-02T00:00:00+00:00</updated><id>https://sanjanad1024.github.io/blog/2024/adjoint-compact</id><content type="html" xml:base="https://sanjanad1024.github.io/blog/2024/adjoint-compact/"><![CDATA[<p>While studying for the 18.102 (Introduction to Functional Analysis) final this semester, I came across the following theorem, which I think is quite cool (see below for the relevant definitions).</p> <div class="theorem"> Let $X$ and $Y$ be Banach spaces, and suppose that $T \colon X \to Y$ is a compact bounded linear operator. Then its adjoint $T^* \colon Y^* \to X^*$ is also compact. </div> <p>In this post, I’ll explain a proof, based on the first answer to this <a href="https://math.stackexchange.com/questions/41432/easy-proof-adjointcompact-compact" target="_blank">Math StackExchange post</a>.</p> <h1 id="definitions-and-setup">Definitions and setup</h1> <p>First, here are some preliminary definitions (stated here for reference). Throughout this post, we work over $\mathbb{C}$.</p> <ul> <li> <div class="sd-hidden" desc="Normed linear spaces and Banach spaces"> <div class="definition"> We say $X$ is a <span class="vocab">normed linear space</span> if $X$ is a vector space together with a norm $\lVert \bullet \rVert$ satisfying the following three properties: <ul> <li> $\lVert x \rVert \geq 0$ for all $x \in X$, with equality if and only if $x = 0$.</li> <li> Homogeneity &mdash; $\lVert \alpha x\rVert = \lvert \alpha \rvert \lVert x\rVert$ for all $x \in X$ and $\alpha \in \mathbb{C}$. </li> <li> The triangle inequality &mdash; $\lVert x + y \rVert \leq \lVert x \rVert + \lVert y \rVert$ for all $x, y \in X$. </li> </ul> </div> <div class="definition"> A <span class="vocab">Banach space</span> is a complete normed linear space. </div> If $X$ is a normed linear space, then the norm induces a metric $d(x, y) = \lVert x - y\rVert$. When we say that $X$ is complete, we're referring to this norm. </div> </li> <li> <div class="sd-hidden" desc="Bounded linear operators"> Next, we'll state a few definitions regarding linear operators between normed linear spaces. <div class="definition"> If $X$ and $Y$ are normed linear spaces and $T \colon X \to Y$ is a linear operator, we say $T$ is <span class="vocab">bounded</span> if there exists a constant $c$ such that $\lVert Tx\rVert \leq c\lVert x\rVert$ for all $x \in X$. </div> <div class="definition"> If $T$ is a bounded linear operator, we define its <span class="vocab">operator norm</span> as \[\lVert T\rVert = \sup_{x \neq 0} \frac{\lVert Tx\rVert}{\lVert x\rVert}.\] </div> By homogeneity, we could equivalently define $\lVert T\rVert$ as $\sup_{\lVert x\rVert = 1} \lVert Tx\rVert$. <div class="fact"> If $X$ and $Y$ are normed linear spaces, then the set of bounded linear operators $T \colon X \to Y$ form a normed linear space as well (under the operator norm), which we denote by $\mathcal{B}(X, Y)$. Furthermore, if $Y$ is Banach, then so is $\mathcal{B}(X, Y)$. </div> </div> </li> <li> <div class="sd-hidden" desc="Dual spaces and adjoints"> <div class="definition"> For a normed linear space $X$, its <span class="vocab">dual space</span> $X^\ast$ is defined as $\mathcal{B}(X, \mathbb{C})$. We refer to elements of $X^\ast$ (which are bounded linear functions $f \colon X \to \mathbb{C}$) as <span class="vocab">functionals</span>. </div> Note that since $\mathbb{C}$ is Banach (i.e., complete), so is $X^*$ (for any normed linear space $X$). <div class="definition"> For any bounded linear operator $T \colon X \to Y$, we define its <span class="vocab">adjoint operator</span> $T^* \colon Y^* \to X^*$ as the map such that $(T^\ast g)(x) = g(Tx)$ for all $g \in Y^\ast$ and $x \in X$. </div> In other words, we're defining $T^\ast$ as the map sending each bounded linear operator $g \colon Y \to \mathbb{C}$ (which is an element of $Y^\ast$) to the bounded linear operator $g \circ T \colon X \to \mathbb{C}$ (an element of $X^\ast$). <div class="fact"> If $T$ is a bounded linear operator, then so is $T^\ast$. </div> </div> </li> <li> <div class="sd-hidden" desc="Compactness in metric spaces"> <div class="definition"> For a metric space $X$, we say a subset $M \subseteq X$ is <span class="vocab">compact</span> if every open cover of $M$ has a finite subcover &mdash; in other words, for any collection $\mathcal{U}$ of open sets whose union contains $M$, we can find a <em>finite</em> subcollection of $\mathcal{U}$ whose union still contains $M$. </div> <div class="definition"> For a metric space $X$, we say a subset $M \subseteq X$ is <span class="vocab">sequentially compact</span> if every sequence $(x_n) \subseteq M$ has a subsequence converging to some point in $M$. </div> <div class="fact"> Compactness and sequential compactness are equivalent (for metric spaces). </div> The reason there's two separate terms is because both can be defined in greater generality for general topological spaces, and there they're not necessarily equivalent. In this post, we'll make use of both notions. </div> </li> </ul> <p>The main focus of this post is a result about compact operators, so now we’ll discuss what it means for an operator to be compact. (We’ll only consider the case where we’re working with bounded linear operators between two Banach spaces.)</p> <div class="definition"> For Banach spaces $X$ and $Y$, we say a bounded linear operator $T \colon X \to Y$ is <span class="vocab">compact</span> if for every bounded sequence $(x_n) \subseteq X$, its image $(Tx_n) \subseteq Y$ has a convergent subsequence. </div> <p>There’s an equivalent characterization of when an operator is compact, which may make it more clear where the name comes from. (In the post, we’re going to make use of both characterizations.)</p> <div class="lemma"> For Banach spaces $X$ and $Y$, an operator $T \colon X \to Y$ is compact if and only if for every bounded subset $M \subseteq X$, its image $TM \subseteq Y$ has compact closure. </div> <p>We’ll use $\operatorname{Cl}(M)$ to refer to the closure of a subset $M$, so the condition here states that $\operatorname{Cl}(TM) \subseteq Y$ is compact (as a metric space) whenever $M \subseteq X$ is bounded. We’ll refer to this condition as $(\star)$.</p> <div class="proof"> First, the backwards direction is pretty direct &mdash; suppose that $T$ has the property $(\star)$. Consider some bounded sequence $(x_n) \subseteq X$, and let $M = \{x_n \mid n \in \mathbb{N}\}$ be the set it forms (so that $M$ is bounded). Then $(Tx_n)$ is contained in $\operatorname{Cl}(TM)$, which is compact by $(\star)$; so it must have a convergent subsequence. <br/><br/> Now we'll prove the forwards direction &mdash; we'll suppose that $T$ is compact (as in the original definition) and show that it satisfies $(\star)$. Fix some bounded set $M \subseteq X$. We want to show $\operatorname{Cl}(M)$ is compact, and we'll do so by showing that it's <em>sequentially</em> compact &mdash; i.e., that any $(y_n) \subseteq \operatorname{Cl}(TM)$ has a convergent subsequence whose limit is also in $\operatorname{Cl}(TM)$. <br/><br/> First, to apply the compactness of $T$, we really want to be working with a sequence in $TM$ rather than its closure, so the first step is to replace $(y_n) \subseteq \operatorname{Cl}(TM)$ with a sequence $(y_n') \subseteq TM$ that's 'close' to it &mdash; since $TM$ is dense in its closure, for each $n \in \mathbb{N}$ we can find some $y_n' \in TM$ with $\lVert y_n - y_n'\rVert &lt; \frac{1}{n}$, and since $y_n' \in TM$, we can find $x_n \in M$ with $y_n' = Tx_n$. <br/><br/> And now the sequence $(x_n)$ is bounded (as it's contained in $M$, which is bounded), so by the compactness of $T$, its image $(y_n') = (Tx_n) \subseteq Y$ must have a convergent subsequence. And since $\lVert y_n - y_n'\rVert \to 0$ (as $n \to \infty$), this means the corresponding subsequence of $(y_n)$ is convergent as well. <br/><br/> Finally, we've found a subsequence of $(y_n)$ that converges to <em>some</em> point $y \in Y$. But since $\operatorname{Cl}(TM)$ is closed and each $y_n$ is in $\operatorname{Cl}(TM)$, this means $y \in \operatorname{Cl}(TM)$ as well. This proves that $\operatorname{Cl}(TM)$ is sequentially compact (and therefore compact), so we're done. </div> <h1 id="the-proof">The proof</h1> <p>We’ll now prove the main theorem. Suppose that $T \colon X \to Y$ is compact. Our goal is to show that $T^\ast \colon Y^\ast \to X^\ast$ is compact as well; using the definition of a compact operator, this means we’ve got some bounded sequence $(g_n) \subseteq Y^\ast$, and we want to show that $(T^\ast g_n) \subseteq X^\ast$ has a convergent subsequence.</p> <h2 id="step-1--reformulating-the-conclusion">Step 1 — reformulating the conclusion</h2> <p>First, we’re going to find a sufficient condition for a sequence $(T^\ast g_n) \subseteq X^\ast$ to be convergent that’s more convenient to work with (and we’re going to find a subsequence satisfying this property instead).</p> <div class="claim"> Let $B = \{x \in X \mid \lVert x\rVert \leq 1\}$ be the closed unit ball in $X$, and let $K = \operatorname{Cl}(TB)$ be the closure of its image in $Y$. Suppose that $(g_n) \subseteq Y^\ast$ is a sequence of functionals such that for every $\varepsilon &gt; 0$, there exists $N$ such that for all $m, n \geq N$ we have \[\sup_{y \in K} \lvert g_m(y) - g_n(y)\rvert \leq \varepsilon.\] Then the sequence $(T^\ast g_n) \subseteq X^\ast$ is convergent. </div> <div class="proof"> First, it's enough to show that $(T^\ast g_n)$ is <em>Cauchy</em> &mdash; this automatically implies it's convergent, as $X^\ast$ is complete. So we want to show that for every $\varepsilon &gt; 0$, there exists $N$ such that for all $m, n \geq N$ we have $\lVert T^\ast g_m - T^\ast g_n\rVert \leq \varepsilon$. But for any functional $g$, we have \[\lVert T^\ast g\rVert = \sup_{\lVert x\rVert = 1} \lvert (T^\ast g)(x)\rvert = \sup_{\lVert x\rVert = 1} \lvert g(Tx)\rvert \leq \sup_{y \in K} \lvert g(y)\rvert\] (since if $\lVert x\rVert = 1$, then $Tx \in TB \subseteq K$). Taking $g = g_m - g_n$, we get \[\lVert T^\ast g_m - T^\ast g_n\rVert \leq \sup_{y \in K} \lvert g_m(y) - g_n(y)\rvert\] for all $m$ and $n$; so if we know the right-hand side is at most $\varepsilon$ for all $m, n \geq N$, then so is the left-hand side (with the same value of $N$). </div> <p>Note that $K$ is compact (because $T$ is compact, and $B \subseteq X$ is bounded) — this is the one place where we’re going to use the compactness of $T$. (After this, $T$ will basically disappear from the picture.)</p> <h2 id="step-2--finding-a-sequence-of-representatives">Step 2 — finding a sequence of representatives</h2> <p>Now our goal is to find a subsequence $(g_n’) \subseteq (g_n)$ satisfying the condition of <span class="crossref">Claim 4</span>. The first step towards doing so is finding a countable sequence of ‘representatives’ $y_1$, $y_2$, $\ldots$ for $K$ with certain nice properties. The reason for this is that we’re then going to construct our subsequence $(g_n’) \subseteq (g_n)$ such that it converges pointwise at each of these representatives $y_i$, and argue that this implies the desired conclusion.</p> <div class="claim"> There exists a sequence $(y_i) \subseteq K$ with the property that for every $\varepsilon &gt; 0$, there exists $N$ such that every $y \in K$ is within $\varepsilon$ of one of $y_1$, $\ldots$, $y_N$. </div> <div class="proof"> First, the compactness of $K$ implies that for every $n \in \mathbb{N}$, we can cover $K$ with a finite number of balls of radius $\frac{1}{n}$ &mdash; i.e., there exists a finite set of points $S_n \subseteq K$ such that $\bigcup_{y \in S_n} \mathbb{B}(z, \frac{1}{n}) \supseteq K$. (This is because the balls $\mathbb{B}(y, \frac{1}{n})$ over <em>all</em> points $y \in K$ form an open cover of $K$, and by the compactness of $K$, this open cover must have a finite subcover.) <br/><br/> Then we can take $(y_i)$ to consist of the points in $S_1$, then $S_2$, then $S_3$, and so on (in that order). <br/><br/> <center><img src="/assets/img/compact3.png" width="600" height="auto"/></center> <br/><br/> This will have the desired property &mdash; given any $\varepsilon &gt; 0$, we can fix $n \in \mathbb{N}$ such that $\frac{1}{n} \leq \varepsilon$. Then every $y \in K$ is within $\varepsilon$ of some point in $S_n$, so we can take $N = \lvert S_1\rvert + \cdots + \lvert S_n\rvert$. </div> <h2 id="step-3--defining-the-subsequence">Step 3 — defining the subsequence</h2> <p>Now we’re going to define our subsequence $(g_n’) \subseteq (g_n)$, so that it has the following property.</p> <div class="lemma"> There is a subsequence $(g_n') \subseteq (g_n)$ such that for each $i \in \mathbb{N}$, the sequence $(g_n'(y_i)) \subseteq \mathbb{C}$ is convergent (as $n \to \infty$). </div> <div class="proof"> We're going to use a diagonalization argument &mdash; the idea is that we'll first restrict to a subsequence along which $(g_n(y_1))$ converges, then further restrict to a subsequence along which $(g_n(y_2))$ converges, and so on; and in the end, we'll take the 'diagonal' of all these subsequences. <br/><br/> First, we know the sequence $(g_n)$ is bounded (by assumption), so there's some $c$ such that $\lVert g_n\rVert \leq c$ for all $n \in \mathbb{N}$. This means $\lvert g_n(y_1)\rvert \leq c\lVert y_1\rVert$ for all $n$, so the sequence $(g_n(y_1))$ is a bounded sequence in $\mathbb{C}$, which means it has a convergent subsequence (as it's a sequence in a compact subset of $\mathbb{C}$). So we can define a subsequence $(g_{n1}) \subseteq (g_n)$ such that $(g_{n1}(y_1))$ is convergent. <br/><br/> Next, we're going to further restrict this subsequence to deal with $y_2$ &mdash; we have $\lvert g_{n1}(y_2)\rvert \leq c\lVert y_2\rVert$ for all $n$, so the sequence $(g_{n1}(y_2))$ is also a bounded sequence in $\mathbb{C}$, which means it also has a convergent subsequence. So we can define a subsequence $(g_{n2}) \subseteq (g_{n1})$ such that $(g_{n2}(y_2))$ is convergent. <br/><br/> And we can continue doing this to get a nested list of subsequences $(g_n) \supseteq (g_{n1}) \supseteq (g_{n2}) \supseteq \cdots$ such that for each fixed $i \in \mathbb{N}$, the sequence $(g_{ni}(y_i)) \subseteq \mathbb{C}$ is convergent. <br/><br/> <center><img src="/assets/img/compact1.png" width="600" height="auto"/></center> <br/><br/> Finally, we define the subsequence $(g_n')$ by $g_n' = g_{nn}$ for each $n \in \mathbb{N}$ &mdash; so we're essentially taking the 'diagonal' of our list of nested subsequences. <br/><br/> <center><img src="/assets/img/compact2.png" width="600" height="auto"/></center> <br/><br/> (For example, in the above picture our sequence $(g_n')$ begins with $g_1$, $g_3$, $g_9$, $\ldots$.) <br/><br/> To see that this works, fix some $i \in \mathbb{N}$. Then if we take the sequence $(g_n'(y_i))$ and remove the first $i - 1$ terms, the resulting sequence is a subsequence of $(g_{ni}(y_i))$, which is convergent by construction; so this means $(g_n'(y_i))$ is convergent as well. </div> <h2 id="step-4--concluding">Step 4 — concluding</h2> <p>Finally, we’re ready to conclude — we’ll use the fact that $(y_i)$ forms a nice sequence of representatives for $K$ (from <span class="crossref">Claim 5</span>) together with the fact that $(g_n’)$ converges pointwise at each $y_i$ (from <span class="crossref">Lemma 6</span>) to conclude that $(g_n’)$ satisfies the condition described in <span class="crossref">Claim 4</span>.</p> <div class="claim"> For every $\varepsilon &gt; 0$, there exists $N$ such that for all $m, n \geq N$ and all $y \in K$, we have \[\lvert g_m'(y) - g_n'(y)\rvert \leq \varepsilon.\] </div> <div class="proof"> First, let $c$ be a constant such that $\lVert g_n\rVert \leq c$ for all $n \in \mathbb{N}$ (such $c$ exists because $(g_n)$ is bounded). <br/><br/> Now fix $\varepsilon &gt; 0$. Then using <span class="crossref">Claim 5</span>, we can first find a constant $M$ such that for every $y \in K$, there is some $i \in \{1, \ldots, M\}$ with $\lVert y - y_i\rVert \leq \frac{\varepsilon}{4c}$; fix this value of $M$. <br/><br/> Then for each $i \in \{1, \ldots, M\}$, we know the sequence $(g_n'(y_i))$ is convergent and therefore Cauchy (by <span class="crossref">Lemma 6</span>), so there is some $N_i$ such that for all $m, n \geq N_i$ we have $\lVert g_m'(y_i) - g_n'(y_i)\rVert \leq \frac{\varepsilon}{2}$. <br/><br/> Finally, let $N = \max\{N_1, \ldots, N_M\}$. To see that this has the desired property, consider any $y \in K$, and fix $y_i$ with $i \in \{1, \ldots, M\}$ such that $\lVert y - y_i\rVert \leq \frac{\varepsilon}{4c}$. Then we can write \[\lVert g_m'(y) - g_n'(y)\rVert \leq \lVert g_m'(y) - g_m'(y_i)\rVert + \lVert g_m'(y_i) - g_n'(y_i)\rVert + \lVert g_n'(y_i) - g_n'(y)\rVert\] by the triangle inequality. (The intuition here is that we know $g_m'$ and $g_n'$ should be 'close' at $y_i$, and we know $y$ should be close to $y_i$, so we should be able to bound each of these terms.) <br/><br/> For the first term, we have \[\lVert g_m'(y) - g_m'(y_i)\rVert \leq \lVert g_m'\rVert \lVert y - y_i\rVert \leq c\cdot \frac{\varepsilon}{4c} = \frac{\varepsilon}{4}\] (the first inequality is by the definition of the operator norm of $g_m'$). We can do the same for the last term to get that it's at most $\frac{\varepsilon}{4}$ as well. Finally, for the middle term, we have \[\lVert g_m'(y_i) - g_n'(y_i)\rVert \leq \frac{\varepsilon}{2},\] since we know $m, n \geq N \geq N_i$ and we defined $N_i$ to guarantee this inequality holds. <br/><br/> Putting these together gives that \[\lVert g_m'(y) - g_n'(y)\rVert \leq \frac{\varepsilon}{4} + \frac{\varepsilon}{2} + \frac{\varepsilon}{4} = \varepsilon\] for all $m, n \geq N$ and $y \in K$, as desired. </div> <p>This concludes the proof of <span class="crossref">Theorem 1</span> — we’ve started with an arbitrary bounded sequence $(g_n) \subseteq Y^*$ and obtained a subsequence $(g_n’) \subseteq (g_n)$ satisfying the condition of <span class="crossref">Claim 4</span> (using <span class="crossref">Claim 5</span> to choose a nice set of representatives, <span class="crossref">Lemma 6</span> to find a subsequence converging pointwise at each representative, and finally using the triangle inequality to get the conclusion, as stated in <span class="crossref">Claim 7</span>), which by <span class="crossref">Claim 4</span> means that the sequence $(T^\ast g_n’)$ is convergent.</p>]]></content><author><name></name></author><category term="analysis,"/><category term="functional-analysis"/><summary type="html"><![CDATA[A proof of the theorem from functional analysis that the adjoint of a compact linear operator between Banach spaces is also compact.]]></summary></entry><entry><title type="html">Perfect Power Polynomials</title><link href="https://sanjanad1024.github.io/blog/2022/powerpoly/" rel="alternate" type="text/html" title="Perfect Power Polynomials"/><published>2022-06-04T00:00:00+00:00</published><updated>2022-06-04T00:00:00+00:00</updated><id>https://sanjanad1024.github.io/blog/2022/powerpoly</id><content type="html" xml:base="https://sanjanad1024.github.io/blog/2022/powerpoly/"><![CDATA[<p>(This post is based on a talk I gave at my high school’s math club.)</p> <h1 id="introduction">Introduction</h1> <p>In this post, we’ll answer the following question.</p> <div class="question"> Which integer-coefficient polynomials $P$ have the property that $P(n)$ is a perfect power for every integer $n$? </div> <p>To be more precise, we’ll use the following definitions. (Some notational conventions: we use $\mathbb{Z}[x]$ and $\mathbb{Q}[x]$ to denote the sets of polynomials with integer coefficients and rational coefficients, respectively, and we use $\nu_p(n)$ to denote the greatest power of $p$ dividing $n$.)</p> <div class="definition"> We say an integer $n$ is a <em>perfect power</em> if $n = m^k$ for integers $m$ and $k$ with $k &gt; 1$. </div> <div class="definition"> We say a polynomial $P \in \mathbb{Z}[x]$ is a <em>perfect power</em> if $P(x) = Q(x)^k$ for some polynomial $Q(x) \in \mathbb{Z}[x]$ and some integer $k &gt; 1$. </div> <p>If a polynomial $P$ is a perfect power, then $P(n)$ is a perfect power for every integer $n$ — we can write $P(n)$ as $Q(n)^k$. So it’s natural to ask whether the converse is true — if we know that $P(n)$ is a perfect power for every integer $n$, then must $P$ <em>itself</em> be a perfect power? It turns out the answer is yes.</p> <div class="theorem"> If a polynomial $P \in \mathbb{Z}[x]$ has the property that $P(n)$ is a perfect power for all integers $n$, then $P$ itself must be a perfect power. </div> <p>In this post, we’ll explain some theory regarding the behavior of integer-coefficient polynomials, and use that theory to prove this statement.</p> <h2 id="proof-idea">Proof Idea</h2> <p>First we’ll give a high-level overview of the proof. We’ll start by factoring our polynomial $P$ as \[P(x) = c\cdot Q_1(x)^{e_1}\cdot Q_2(x)^{e_2}\cdots Q_k(x)^{e_k}\] for some distinct irreducible polynomials $Q_i(x) \in \mathbb{Z}[x]$ and some integer $c$. (Unique factorization <em>does</em> hold in $\mathbb{Z}[x]$, but if you are uncomfortable with this, then you can perform the factorization in $\mathbb{Q}[x]$ and clear denominators, allowing $c$ to be rational — this doesn’t affect the proof at all.)</p> <p>The main idea of the proof is to construct distinct primes $p_1$, $p_2$, $\ldots$, $p_k$ and some integer $n$, such that for each $i$, we have $\nu_{p_i}(Q_i(n)) = 1$ and $\nu_{p_i}(Q_j(n)) = 0$ for $j \neq i$ — we’re constructing one prime $p_i$ for each factor $Q_i$, and trying to use these primes to ‘pull out’ the exponents $e_i$. (We also want our primes $p_i$ to not divide $c$.) If we can do so, then we have $\nu_{p_i}(P(n)) = e_i$ for each $i$. Since $P(n)$ is the $d$th power of an integer for some $d &gt; 1$, then we must have $\gcd(e_1, \ldots, e_k) = d$, and therefore we can conclude that $P$ is the $d$th power of a polynomial.</p> <p>We’ll now see some facts about integer-coefficient polynomials that allow us to find such primes $p_1$, $\ldots$, $p_k$.</p> <h1 id="bounded-gcds">Bounded GCDs</h1> <p>In this section, we’ll prove the following lemma.</p> <div class="lemma"> Let $P, Q \in \mathbb{Z}[x]$ be two relatively prime polynomials. Then there is some constant $c$ such that $\gcd(P(n), Q(n)) \leq c$ for all integers $n$. </div> <p>In order to prove this, we can use the Euclidean Algorithm.</p> <h2 id="euclidean-algorithm-for-polynomials">Euclidean Algorithm for Polynomials</h2> <p>If we’re trying to find the gcd of two <em>integers</em> $a$ and $b$, we know that $\gcd(a, b) = \gcd(a - kb, b)$ for any integer $k$. So then we can repeatedly replace the larger number with its remainder mod the smaller number until we end up with $\gcd(0, d) = d$ for some nonzero integer $d$. For example, \[\gcd(20, 14) = \gcd(6, 14) = \gcd(6, 2) = \gcd(0, 2) = 2.\]</p> <p>We can do the same thing with polynomials with <em>rational</em> coefficients — given any two polynomials with rational coefficients, in order to find their gcd (the gcd of two rational-coefficient polynomials is defined as the highest-degree monic polynomial (also with rational coefficients) which divides both of them in $\mathbb{Q}[x]$), we can repeatedly replace the higher-degree one with its remainder mod the lower-degree one, until we end up with $\gcd(0, R) = R$ for some polynomial $R \in \mathbb{Q}[x]$. (The reason we’re using <em>rational</em> coefficients rather than integer coefficients here is because we can’t necessarily do polynomial division in $\mathbb{Z}[x]$ — if we’re dividing by a non-monic polynomial, we may introduce fractions.)</p> <div class="example"> Find $\gcd(x^3 + 3x + 1, x^2 - 2)$. </div> <div class="proof"> First, we can use polynomial division to get \[x^3 + 3x + 1 = (x^2 - 2)x + 5x + 1,\] which means \[\gcd(x^3 + 3x + 1, x^2 - 2) = \gcd(5x + 1, x^2 - 2).\] Now we can perform polynomial division again to get \[x^2 - 2 = (5x + 1)(\tfrac{1}{5}x - \tfrac{1}{25}) - \tfrac{49}{25}.\] So we have \[\gcd(5x + 1, x^2 - 2) = \gcd(5x + 1, \tfrac{49}{25}) = \gcd(0, \tfrac{49}{25}) = 1.\] </div> <p>This process successfully terminates (meaning that we can make one term $0$) for the same reason that it does in the integers — every step decreases the degree of our polynomials.</p> <p>Similarly to the case of integers, every polynomial we write down in this process is a linear combination of $P$ and $Q$ (where the coefficients are rational-coefficient polynomials). In the integer case, this gives us Bezout’s Theorem; similarly, here this means \[\gcd(P, Q) = A\cdot P + B\cdot Q\] for some rational-coefficient polynomials $A$ and $B$.</p> <h2 id="proving-the-lemma">Proving the Lemma</h2> <p>Now this gives us the tools to prove our lemma that relatively prime polynomials have bounded gcds.</p> <div class="proof"> Since $P$ and $Q$ are relatively prime, we have $\gcd(P, Q) = 1$, which means \[1 = A\cdot P + B\cdot Q\] for rational-coefficient polynomials $A$ and $B$. This means \[d = A^*\cdot P + B^* \cdot Q\] for <em>integer</em>-coefficient polynomials $A^*$ and $B^*$, by clearing denominators. So then $\gcd(P(n), Q(n))$ must divide $d$, for all integers $n$. </div> <h1 id="schurs-theorem">Schur’s Theorem</h1> <p>Earlier, we wrote our polynomial as \[P(x) = c\cdot Q_1(x)^{e_1}\cdot Q_2(x)^{e_2}\cdots Q_k(x)^{e_k},\] and we hoped to find primes $p_1$, $p_2$, $\ldots$, $p_k$ such that each $p_i$ functions as a sort of indicator for $Q_i(n)$ — we want $p_i$ to divide $Q_i(n)$, but not $Q_j(n)$ for any $j \neq i$.</p> <p>Since all the $Q_i$ are relatively prime as polynomials, we know that their pairwise gcds are all bounded. So if we choose our primes to be large enough, then if $p_i$ divides $Q_i(n)$ it <em>definitely</em> can’t divide $Q_j(n)$ for any other $j$.</p> <p>But what if we <em>can’t</em> choose a large enough prime — what if all primes which divided $Q_i(n)$ (over all $n$) were less than $100$, for instance? This is where Schur’s Theorem comes in.</p> <div class="theorem" text="Schur&apos;s Theorem"> If $P \in \mathbb{Z}[x]$ is a nonconstant polynomial, then there are infinitely many primes which divide $P(n)$ for some integer $n$. </div> <div class="proof"> Assume for contradiction that the only primes dividing $P(n)$, over all $n$, are $p_1$, $p_2$, $\ldots$, $p_r$. Now the idea is to construct a bunch of values of $n$ which have the same value of $P(n)$, by imposing conditions mod powers of these primes. <br/><br/> Fix some $a$ for which $P(a) \neq 0$, and for each prime $p_i$, let $\nu_{p_i}(P(a)) = e_i$. Then construct \[n \equiv a \pmod{p_i^{e_i + 1}}\] for all $i$ (this is possible by the Chinese Remainder Theorem). Then we have \[P(n) \equiv P(a) \pmod{p_i^{e_i + 1}}\] for all $i$ as well, which means \[\nu_{p_i}(P(n)) = \nu_{p_i}(P(a))\] for all $i$. But since these are the only primes which can divide either $P(n)$ or $P(a)$, this means we must have $|P(n)| = |P(a)|$. <br/><br/> So then there is some value which $P$ takes infinitely many times &mdash; namely, either $P(a)$ or $-P(a)$ &mdash; which means $P$ must be constant. </div> <p>So by Schur’s Theorem, we know that we can find arbitrarily large (and distinct) primes $p_1$, $p_2$, $\ldots$, $p_k$, and positive integers $n_1$, $n_2$, $\ldots$, $n_k$, such that $p_i \mid Q_i(n_i)$ for each $i$. We can then combine these $n_i$ into one $n$ by choosing \[n \equiv n_i \pmod{p_i}\] for all $i$ (which is possible by the Chinese Remainder Theorem). Then by the fact that gcds are bounded, we know that $p_i$ doesn’t divide $Q_j(n)$ for any $j \neq i$.</p> <h1 id="hensels-lemma">Hensel’s Lemma</h1> <p>We’ve <em>almost</em> constructed everything we wanted to — we now have our indicator primes, and we’ve shown that they aren’t affected by any factor except the one they’re assigned to. But there’s one thing missing. It’s not enough to know that $p_i \mid Q_i(n)$ — we need to know that <em>exactly one</em> power of $p_i$ divides $Q_i(n)$, or in other words, $\nu_{p_i}(Q_i(n)) = 1$. (This is so that we can get that the power of $p_i$ in the prime factorization of $P(n)$ is exactly $e_i$.)</p> <p>So we want to try actually choosing the $n_i$ mod $p_i^2$ such that $Q_i(n_i)$ is divisible by $p_i$, but not $p_i^2$. In order to do this, we’ll use Hensel’s Lemma:</p> <div class="theorem" desc="Hensel&apos;s Lemma"> Let $P \in \mathbb{Z}[x]$ be a polynomial, and $p$ a prime and $n_1$ an integer. If $p \nmid P'(n_1)$, then for any positive integer $e$ and any $c \equiv P(n_1) \pmod{p}$, we can find some $n_e \equiv n_1 \pmod{p}$ such that \[P(n_e) \equiv c \pmod{p^e}.\] </div> <p>So essentially, Hensel’s Lemma states that if we can solve a polynomial equation mod $p$, and at that point the <em>derivative</em> isn’t divisible by $p$, then we can solve it mod any power of $p$.</p> <p>Before we see the proof, we’ll first look at a specific example:</p> <div class="example"> Show that if $p &gt; 2$ and there exists $n_1$ such that $n_1^2 + 2 \equiv 0 \pmod{p}$, then for every integer $a$, there exists $n \equiv n_1 \pmod{p}$ such that $n^2 + 2 \equiv ap \pmod{p^2}$. </div> <div class="proof"> Let $n = n_1 + pt$, for some integer $t$. Then we have \[n^2 + 2 = n_1^2 + 2pn_1t + p^2t^2 + 2 \equiv n_1^2 + 2 + 2pn_1t \pmod{p^2}.\] We know $n_1^2 + 2$ is some multiple of $p$, and since $p \nmid 2n_1$, then $2pn_1t$ must run over all possible multiples of $p$. So as $t$ varies, $n^2 + 2$ must cover all multiples of $p$ mod $p^2$. </div> <p>The proof in the general case is essentially the same:</p> <div class="proof" text="Proof of Hensel&apos;s Lemma"> Induct on $e$ &mdash; in the base case $e = 1$, there is nothing to prove. Now choose $n_{e - 1} \equiv n_1 \pmod{p}$ such that $P(n_{e - 1}) \equiv c \pmod{p^{e - 1}}$, using the inductive hypothesis. Now let $n_e = n_{e - 1} + p^{e - 1}t$ for any integer $t$. <br/><br/> Let $P(x) = a_nx^n + a_{n - 1}x^{n - 1} + \cdots + a_1x + a_0$. Then we have \[(n_{e - 1} + p^{e - 1}t)^i \equiv n_{e - 1}^i + ip^{e - 1}tn_{e - 1}^{i - 1} \pmod{p^e}\] by the Binomial Theorem (every other term is divisible by $p^e$, since $2(e - 1) \geq e$). So this means \[P(n_e) \equiv P(n_{e - 1}) + p^{e - 1}\cdot P'(n_{e - 1})t \pmod{p^e}.\] Since $P'(n_{e - 1})$ is not divisible by $p$, then the second term must cover all multiples of $p^{e - 1}$ mod $p^e$, which means over all values of $t$, we must be able to get $P(n_e) \equiv c \pmod{p^e}$. </div> <p>Now in our case, we know $Q_i$ and $Q_i’$ are relatively prime, as polynomials (since the $Q_i$ are irreducible), so their gcds are again bounded. So as long as we chose our primes $p_i$ to be large enough, we know that if $p_i \mid Q_i(n_i)$, then $p_i \nmid Q_i’(n_i)$. So we can actually choose $n_i$ mod $p_i^2$ such that $Q_i(n_i) \equiv p_i \pmod{p_i^2}$.</p> <p>Then, by the Chinese Remainder Theorem we can again choose $n$ such that $n \equiv n_i \pmod{p_i^2}$ for all $i$. So for this choice of $n$, we have $\nu_{p_i}(Q_i(n)) = 1$ for all $i$.</p> <h1 id="conclusion">Conclusion</h1> <p>We’ve written \[P(x) = cQ_1(x)^{e_1}Q_2(x)^{e_2}\cdots Q_k(x)^{e_k}\] for irreducible polynomials $Q_i$, and chosen huge distinct primes $p_1$, $p_2$, $\ldots$, $p_k$ and a positive integer $n$ such that $\nu_{p_i}(Q_i(n)) = 1$ and $\nu_{p_i}(Q_j(n)) = 0$ for all $j \neq i$.</p> <p>This means for each $i$, we have \[\nu_{p_i}(P(n)) = e_i.\] But we know $P(n)$ is a $d$th power of an integer for some $d &gt; 1$. So $d$ must divide $e_i$ for all $i$. Finally, then $Q_1(n)^{e_1}\cdots Q_k(n)^{e_k}$ is a $d$th power, so $c$ must be a $d$th power as well.</p> <p>So $P$ is a $d$th power of a polynomial, and is therefore a perfect power.</p>]]></content><author><name></name></author><category term="number-theory"/><category term="olympiad"/><summary type="html"><![CDATA[Which integer-coefficient polynomials only attain values which are perfect powers?]]></summary></entry><entry><title type="html">Tournament of Towns</title><link href="https://sanjanad1024.github.io/blog/2022/tournament-of-towns/" rel="alternate" type="text/html" title="Tournament of Towns"/><published>2022-02-14T00:00:00+00:00</published><updated>2022-02-14T00:00:00+00:00</updated><id>https://sanjanad1024.github.io/blog/2022/tournament-of-towns</id><content type="html" xml:base="https://sanjanad1024.github.io/blog/2022/tournament-of-towns/"><![CDATA[<p>This winter, I looked at several problems from the <a href="https://www.turgor.ru/en/problems/">Tournament of Towns</a>; here are some that I especially liked. (The last two are among my favorite problems of all time.)</p> <div class="problem" text="ToT Fall 2009 S-A4"> Let $[n]!$ denote the product $1 \times 11 \times 111 \times \cdots \times (11\cdots 1)$ (where the last term has $n$ digits). Prove that $[n + m]!$ is divisible by $[n]!\times[m]!$. </div> <div class="sd-hidden" desc="Solution"> We have $[n]! = \frac{10 - 1}{9} \cdot \frac{10^2 - 1}{9} \cdots \frac{10^n - 1}{9}$, so it suffices to show \[\prod_{i = 1}^n (10^i - 1) \cdot \prod_{i = 1}^m (10^i - 1) \mid \prod_{i = 1}^{m + n} (10^i - 1).\] We'll show that for all primes $p$, the $\nu_p$ of the RHS is at least $\nu_p$ of the LHS. <br/><br/> Suppose $p$ is relatively prime to $10$ (otherwise $p$ cannot divide either side), and let $d = \operatorname{ord}_p 10$. Then only the terms with $i \mid d$ are relevant. If $ad$ and $bd$ are the greatest multiples of $d$ at most $n$ and $m$, respectively, then $(a + b)d \leq n + m$, so it suffices to show \[\sum_{i = 1}^a \nu_p(10^{id} - 1) + \sum_{i = 1}^b \nu_p(10^{id} - 1) \leq \sum_{i = 1}^{a + b} \nu_p(10^{id} - 1).\] But we have \[\nu_p(10^{id} - 1) = \nu_p(10^d - 1) + \nu_p(id) = \nu_p(10^d - 1) + \nu_p(i)\] by LTE. So it suffices to show \[\sum_{i = 1}^a \nu_p(i) + \sum_{i = 1}^b \nu_p(i) \leq \sum_{i = 1}^{a + b} \nu_p(i),\] which is true as $\binom{a + b}{a}$ is an integer. </div> <div class="problem" text="ToT Fall 2011 S-A6"> Prove that for $n \geq 2$, the integer \[1^1 + 3^3 + 5^5 + \cdots + (2^n - 1)^{2^n - 1}\] is a multiple of $2^n$ but not a multiple of $2^{n + 1}$. </div> <div class="sd-hidden" desc="Solution"> Induct on $n$. In the base case $n = 2$, $1^1 + 3^3 = 28$ is a multiple of $4$ but not $8$. <br/><br/> Now suppose $n \geq 3$ and assume this is true for $n - 1$, so \[1^1 + 3^3 + \cdots + (2^{n - 1} - 1)^{2^{n - 1} - 1} \equiv 2^{n - 1} \pmod{2^n}.\] Now note that for all odd $x$, we have \[\nu_2(x^{2^{n - 1}} - 1) = \nu_2(x^2 - 1) + \nu_2(2^{n - 2}) \geq n + 1,\] so then $x^{2^{n - 1}}$ is always $1$ mod $2^{n + 1}$. So then \[(x + 2^{n - 1})^{x + 2^{n - 1}} \equiv (x + 2^{n - 1})^x \equiv x^x + x^x \cdot 2^{n - 1},\] since $2(n - 1) \geq n + 1$. So this means \[1^1 + 3^3 + \cdots + (2^n - 1)^{2^n - 1} \equiv (2 + 2^{n - 1})(1^1 + 3^3 + \cdots + (2^{n - 1} - 1)^{2^{n - 1} - 1}) \pmod{2^{n + 1}}.\] But $2 + 2^{n - 1}$ is even, and the second sum is $2^{n - 1}$ mod $2^n$ by the induction hypothesis, so then the original sum is $2^n$ mod $2^{n + 1}$, as desired. </div> <div class="sd-hidden" desc="Comments"> It makes sense to induct because the sum is pretty intractable on its own, but when you lift from $2^n$ to $2^{n + 1}$, a lot of the terms are either the same or closely related. </div> <div class="problem" text="ToT Fall 2019 S-A7"> Some of the integers $1$, $2$, $\ldots$, $n$ have been colored red so that for each triplet of red numbers $a$, $b$, $c$ (not necessarily distinct), if $a(b - c)$ is a multiple of $n$ then $b = c$. Prove that there are no more than $\varphi(n)$ red numbers. </div> <div class="sd-hidden" desc="Solution"> Let $p_1 &lt; p_2 &lt; \cdots &lt; p_r$ be the primes dividing $n$ which divide some red number, and $q_1$, $q_2$, $\ldots$, $q_s$ the primes dividing $n$ which don't divide any red number. If $r = 0$ then all red numbers are relatively prime to $n$ so there are at most $\varphi(n)$ red numbers; now assume $r \geq 1$. <div class="claim-un"> There are at most $\frac{n}{p_r} \cdot \prod \frac{q_i - 1}{q_i}$ red numbers. </div> <div class="proof"> Let $n = p_r\cdot m$, so $m$ is divisible by each of the $q_i$. Then each residue class mod $m$ contains at most one red number (if $b \equiv c \pmod{m}$, then take $a$ to be a red multiple of $p_r$, so $a(b - c)$ is a multiple of $n$). Additionally, all red numbers are relatively prime to each $q_i$, so all red residues mod $m$ are relatively prime to each $q_i$ as well. So then there are at most $m \prod \frac{q_i - 1}{q_i}$ red numbers. </div> But we have \[\varphi(n) = n \cdot \prod_{i = 1}^r \frac{p_i - 1}{p_i} \cdot \prod_{i = 1}^s \frac{q_i - 1}{q_i}.\] We have the bound \[\prod_{i = 1}^r \frac{p_i - 1}{p_i} \geq \prod_{i = 2}^{p_i} \frac{i - 1}{i} = \frac{1}{p_i},\] so then our bound on the count of red numbers is at most $\varphi(n)$. </div> <div class="sd-hidden" desc="Comments"> You can start by trying small cases &mdash; here the small case is when we only have one prime dividing $n$ with a red multiple, since having none gives exactly $\varphi(n)$. If we look at which residues are allowed then we get exactly $\frac{n}{p}\prod \frac{q - 1}{q}$. Then if we try to go to a more general case, we realize that there <em>isn't</em> really a much better bound we can get (at least, not one that I could find), so we probably still have to use this bound &mdash; and then thinking about size a bit finishes. </div> <div class="problem" text="ToT Spring 2009 S-A7"> Initially, the number $6$ is written on a blackboard. On the $n$th step (for $n \geq 1$), if the number $k$ is on the blackboard, it is replaced with $k + \gcd(k, n)$. Prove that at each step, the number on the blackboard increases either by $1$ or by a prime number. </div> <div class="sd-hidden" desc="Solution"> The first step is $6 \to 7$, the second step is $7 \to 8$, and the third step is $8 \to 9$. <div class="claim-un"> Suppose that after step $n$, the number on the blackboard is $3n$. Then if the number next increases by more than $1$ on step $m$, it increases by a prime and becomes $3m$. </div> <div class="proof"> We have that $m$ is the smallest integer greater than $n$ for which \[\gcd(m, 3n + m - n - 1) = \gcd(m, 2n - 1) &gt; 1.\] But if $p$ is any prime dividing $2n - 1$, then $n \equiv \frac{p + 1}{2} \pmod{p}$, so the smallest $m &gt; n$ with $p \mid m$ is \[m = n + \frac{p - 1}{2}.\] So then the first $m$ for which the gcd is not $1$ is $m = n + \frac{p - 1}{2}$ where $p$ is the smallest prime dividing $2n - 1$, and here the gcd is \[\gcd(2n + p - 1, 2n - 1) = p.\] So step $m$ is \[3n + \frac{p - 1}{2} - 1 \to 3n + \frac{p - 1}{2} - 1 + p = 3m,\] as desired. </div> Since after step $3$ the number is $3\cdot 3$, by induction this means all additions are $1$ or prime, and each prime addition results in $3m$ on the board after turn $m$. </div> <div class="sd-hidden" desc="Comments"> I think this is a pretty rigid problem &mdash; the idea is to try out the process for small values of $n$, and then you notice that there's a pattern: every nontrivial jump ends up in a position $(n, 3n)$, which is surprising but turns out to be not that hard to prove. Maybe it is even expected that the sequence should have some nice property like this, because if there's no good relationship between $n$ and $k$, the problem seems pretty intractable. But even without that heuristic, doing small cases is a good idea. </div> <div class="problem" text="ToT Spring 2011 S-A6"> In every cell of a square table is a number. The sum of the largest two numbers in each row is $a$ and the sum of the largest two numbers in each column is $b$. Prove that $a = b$. </div> <div class="sd-hidden" desc="Solution"> Assume not, so WLOG $a &lt; b$. Label the rows and columns $1$ through $n$. Draw a graph on $n$ vertices and for each column, draw an edge between the row numbers with the two greatest elements of that column (breaking ties arbitrarily) &mdash; multiple edges between two vertices are allowed. <br/><br/> This graph has $n$ vertices and $n$ edges, so it must contain a cycle $(r_1r_2\cdots r_k)$, possibly of length $2$. Suppose edge $r_ir_{i + 1}$ corresponds to column $c_i$. Then if $x(r, c)$ denotes the entry in $(r, c)$, we have \[\sum x(r_i, c_i) + x(r_{i + 1}, c_i) = bn\] by looking at columns. But \[\sum x(r_i, c_{i - 1}) + x(r_i, c_i) \leq an\] by looking at rows (since each term is at most the sum of the two largest numbers in each row). But these are the same sum, so $bn \leq an$, contradiction as $a &lt; b$. </div> <div class="sd-hidden" desc="Comments"> I think this is an arrows problem &mdash; the idea is that you draw a vertical line for each column connecting the two largest numbers in that column, and then you can get a cycle by adding in horizontal lines, which are at most the largest numbers in the rows. <br/><br/> <center><img src="/assets/img/11ttssa6-arrows.png" width="250" height="auto"/></center> </div> <div class="problem" text="ToT Spring 2016 S-A4"> There are $64$ towns in a country, and some pairs of towns are connected by roads but we don't know these pairs. We may choose any pair of towns and find out whether they are connected by a road. Our aim is to determine whether it is possible to travel between any two towns using roads. Prove that there is no algorithm which would enable us to do this in less than $2016$ questions. </div> <div class="sd-hidden" desc="Solution"> Call the person answering us the <em>oracle</em>; we'll show that the oracle can ensure that at every step until the end, the current knowledge is compatible with both the graph being connected and not. <br/><br/> Color an edge red if the oracle answers no, and blue if yes. Define a <em>blob</em> to be a set of vertices $S$ such that all edges in $S$ are drawn, and the blue edges form exactly a tree. Call a position <em>great</em> if it is a collection of blobs (possibly with size 1) so that there are no blue edges between distinct blobs. <br/><br/> Then the oracle can preserve greatness: suppose the position is great, and we query $uv$ with $u$ in blob $S$ and $v$ in blob $T$. The oracle answers no unless every other edge between blobs $S$ and $T$ has been queried (and colored red), in which case he answers yes. <br/><br/> This preserves greatness, as an answer of yes merges the blobs into a bigger blob. The graph will end up connected, but in the last turn there were two blobs, and if the oracle had answered no instead then the graph would be disconnected. So this works. </div> <div class="sd-hidden" desc="Comments"> This is essentially the strategy of saying yes unless forced to say no &mdash; if you have a connected blue subgraph and the other edges between vertices in that subgraph have not been colored red, then you don't ever have to query those edges. Any strategy that preserves greatness and ends with the graph connected should work; in particular saying no unless that would disconnect the graph works as well. </div> <div class="problem" text="ToT Spring 2021 J-A7"> Let $p$ and $q$ be two coprime positive integers. A frog hops along the number line such that on each hop, it moves either $p$ units to the right or $q$ units to the left. Eventually, the frog returns to the initial point. Prove that for every positive integer $d &lt; p + q$, there are two numbers visited by the frog which differ by $d$. </div> <div class="sd-hidden" desc="Solution"> Write the list of jumps $+p$ and $-q$ made by the frog in a circle, so there are $kq$ points on the circle labelled $+p$ and $kp$ points labelled $-q$, in some order. Then it suffices to show that there is some subset of consecutive points on this circle whose sum of labels is exactly $d$. <br/><br/> By Bezout's Theorem there are positive integers $a$ and $b$ with $d = ap - bq$. Look at subsets of exactly $a + b$ consecutive points on the circle. Then it suffices to show there is a subset with at most $a$ points labelled $+p$, and a subset with at least $a$ points labelled $+p$ &mdash; as we walk around the circle (taking the subset starting at each point), the number of points $+p$ in the subset changes by $0$ or $\pm 1$ each step, so by Discrete IVT there then must exist a subset with exactly $a$ points $+p$. <br/><br/> First assume for contradiction that all subsets have at least $a + 1$ points labelled $+p$. Then sum over all $k(p + q)$ subsets. Each point is counted in $a + b$ subsets, and there are exactly $kq$ points $+p$, so then \[k(p + q)(a + 1) \leq kq(a + b).\] This implies $ap - bq \leq -p - q$, contradiction. Similarly, if all subsets have at most $a - 1$ points $+p$, then \[k(p + q)(a - 1) \geq kq(a + b),\] which implies $ap - bq \geq p + q$, contradiction. <br/><br/> So then there exists a subset with at most $a$ points $+p$, and a subset with at least $a$ points $+p$, so by Discrete IVT there exists one with exactly $a$ points $+p$, and this subset sums to exactly $d$. </div> <div class="sd-hidden" desc="Comments"> I really like this problem; I think it's a cool combination of local (look at a frame and shift it) and global (sum over all frames) ideas. I think the main realization is that you want to write the jumps in a circle and look at a <em>fixed</em> set of counts (meaning a specific pair of $a$ and $b$) &mdash; you can sort of motivate this by the fact that you want to look at <em>something</em>, and keeping track of multiple possible solutions would be hard as these seem pretty unrelated to each other (and there sometimes is only one). </div> <div class="problem" text="ToT Spring 2019 S-A7"> Consider lattice paths of finite length which start from $(0, 0)$ and move only up and right, which we call <em>skeletons</em>. For each skeleton, we define a <em>worm</em> consisting of all unit cells in the plane sharing at least one point with the skeleton. (For example, for the path from $(0, 0)$ to $(1, 0)$, the corresponding worm has six cells). Prove that for every integer $n &gt; 2$, the number of worms which can be tiled by dominoes in exactly $n$ ways is equal to $\varphi(n)$. </div> <div class="sd-hidden" desc="Solution"> First, we're going to find a way to recurse the number of tilings of a worm. <br/><br/> Note that the number of ways to tile the worm of skeleton $(0, 0)$ is $2$, since it is a $2 \times 2$ grid. Also, define the number of ways to tile the worm of the empty skeleton as $1$. <br/><br/> Define the <em>twisty part</em> of a skeleton $S$ to be the maximal sequence of points starting with the end for which the skeleton alternates direction, and define the <em>head</em> of a skeleton to be the remainder not in the twisty part (the head may be empty). Here are some examples, with the heads in red and the twisty parts in blue. <br/><br/> <center><img src="/assets/img/19ttssa7-twistypart.png" width="400" height="auto"/></center> <div class="claim-un"> To get the number of ways to tile skeleton $S$, we add the number of ways to tile the skeleton with the last point removed, and the number of ways to tile the head of $S$. </div> <div class="proof"> WLOG the last move in the skeleton is vertical. Now if we place a horizontal domino at the top-right corner, the remaining worm is the worm of the skeleton where we delete the last point of $S$. <br/><br/> <center><img src="/assets/img/19ttssa7-recurse1.png" width="250" height="auto"/></center> Meanwhile, if we place a vertical domino in the top-right corner, then this forces the positions of a bunch of other dominoes. Placing all these other dominoes has the effect of removing the entire twisty part, leaving us with the worm of the head of $S$. <br/><br/> <center><img src="/assets/img/19ttssa7-recurse2.png" width="250" height="auto"/></center> So then the total number of ways to tile $S$ is the sum of these two new skeletons, the one with the last point deleted and the head. </div> Now we can make a big tree, where the top entry is the skeleton $(0, 0)$, and when we go down, left means adding a step right while right means adding a step up. At each node, we write the number of ways to tile that skeleton. We also write the two entries we summed according to the previous claim &mdash; if this was a left child, then we write the skeleton with the last element removed on the left and the head on the right, while if this was a right child then we do the opposite. <br/><br/> <center><img src="/assets/img/19ttssa7-tree.png" width="600" height="auto"/></center> We claim this becomes the Euclidean Algorithm tree where $(a, b)$ has descendants $(a + b, a)$ and $(b, a + b)$. <br/><br/> To show this, if $(a, b)$ with skeleton $S$ is a left child, then $a$ is the number of ways for $S$ with its last point removed, and $b$ is the number of ways for the head of $S$. <br/><br/> Then if we go left one step to get skeleton $T$, then $S$ with $a + b$ ways is $T$ with its last point removed, and $S$ with its last point removed is the head of $T$ (since the twisty part is only the last two points), which gives $(a + b, a)$. <br/><br/> Meanwhile, if we go one step right to get skeleton $T$, then $S$ is still $T$ with its last point removed, but the head of $S$ is the head of $T$ (since we went left to get to $S$, and then right to get to $T$, so $T$ swallows the twisty part of $S$), which gives us $(b, a + b)$. <br/><br/> Similar things happen when $S$ is a right child, which proves that our tree is the Euclidean Algorithm tree. But it's clear that every pair $(a, b)$ with $\gcd(a, b) = 1$ occurs exactly once in this tree (since we can trace it back to $(1, 1)$ uniquely), and no pairs with $\gcd(a, b) &gt; 1$ are in this tree. So then the number of occurrences of $n$ in this tree are the number of ways to write $n = a + b$ with $\gcd(a, b) = 1$, which is $\varphi(n)$. </div> <div class="sd-hidden" desc="Comments"> This is one of my favorite problems of all time. It's a rigid problem &mdash; the point is to start by trying to figure out how to count the number of tilings of a given worm. You can try placing the tile at the end and drawing the dominoes that are forced, and you notice that you end up with a smaller skeleton with a nice characterization. Once you get the recursion you can try drawing a tree to perform the recursion for small worms, and eventually you notice that this is very similar to the Euclidean Algorithm tree &mdash; and you can do the tracking in such a way that it actually <em>becomes</em> the Euclidean Algorithm tree. </div>]]></content><author><name></name></author><category term="olympiad"/><summary type="html"><![CDATA[Some cool problems from the Tournament of Towns.]]></summary></entry></feed>